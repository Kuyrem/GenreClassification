{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ee11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import librosa\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from IPython.display import Audio\n",
    "from audioread.exceptions import NoBackendError\n",
    "import sys\n",
    "import resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37380577",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_list = glob('./fma/data/fma_medium/*/*.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981d2a4c",
   "metadata": {},
   "source": [
    "### Note enough room on my computer to load the entire dataset + data augmentation\n",
    "##### Update: 5k datapoints still too much, maxed out ram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d8a56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_list = sound_list[:1999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c204f33",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b921ef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracks = pd.read_csv('./fma/data/fma_metadata/tracks.csv', low_memory=False)\n",
    "genres = pd.read_csv('./fma/data/fma_metadata/genres.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c1b8c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.rename(columns=tracks.iloc[0], inplace=True)\n",
    "tracks.rename(columns={tracks.columns[0]: tracks.iloc[1][0] }, inplace = True)\n",
    "tracks = tracks.iloc[2:]\n",
    "\n",
    "genres = genres[[\"genre_id\", \"title\"]]\n",
    "genres.set_index(genres.columns[0], inplace=True)\n",
    "glist = genres\n",
    "genres = genres.to_dict()\n",
    "genres = genres['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "245f8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_genre_col(x):\n",
    "    remove_chars = [\"[\", \"]\", \" \"]\n",
    "    for char in remove_chars:\n",
    "        x = x.replace(char, \"\")\n",
    "    if x == \"\":\n",
    "        return None\n",
    "    if \",\" in x:\n",
    "        x = x.split(',')\n",
    "    else:\n",
    "        x = [x]\n",
    "    lst = []\n",
    "    for g in x:\n",
    "        lst.append(genres[int(g)])\n",
    "    return lst\n",
    "\n",
    "def transform_id_col(x):\n",
    "    return x.zfill(6)\n",
    "\n",
    "def extract_id(x):\n",
    "    return x[26:-4]\n",
    "\n",
    "def check_list(x):\n",
    "    return any(elem in x for elem in rmls)\n",
    "\n",
    "def get_list(x):\n",
    "    if any(elem in x for elem in rmls):\n",
    "        c = x\n",
    "        for e in rmls:\n",
    "            if e in x:\n",
    "                c.remove(e)\n",
    "        return c\n",
    "    return 0\n",
    "\n",
    "def check_null(x):\n",
    "    return x == []\n",
    "\n",
    "data = tracks[[\"track_id\", \"bit_rate\", \"genre_top\", \"genres\", \"genres_all\"]]\n",
    "data.loc[:,\"track_id\"] = data[\"track_id\"].apply(transform_id_col)\n",
    "result = list(map(extract_id,sound_list))\n",
    "data = data.loc[data.track_id.isin(result)]\n",
    "data.loc[:,\"genres\"] = data['genres'].apply(transform_genre_col)\n",
    "data.loc[:,\"genres_all\"] = data['genres_all'].apply(transform_genre_col)\n",
    "data['all_genres'] = data.apply(lambda x: list(set(x['genres']).union(set(x['genres_all']), set([x[\"genre_top\"]]))), axis=1)\n",
    "data.drop(columns=['genre_top','genres','genres_all'], inplace=True)\n",
    "\n",
    "enc = MultiLabelBinarizer()\n",
    "encgenres = pd.DataFrame(enc.fit_transform(data[\"all_genres\"]))\n",
    "classes = []\n",
    "\n",
    "for i in range(len(enc.classes_)):\n",
    "    classes.append([i,len(encgenres.loc[encgenres[i]==1])])\n",
    "    \n",
    "rmls = []\n",
    "for i in range(len(enc.classes_)):\n",
    "    \n",
    "    if classes[i][1] < 101:\n",
    "        rmls.append(enc.classes_[i])\n",
    "\n",
    "r = data[\"all_genres\"].apply(check_list)\n",
    "c = data[\"all_genres\"].apply(get_list)\n",
    "n = data[\"all_genres\"].apply(check_null)\n",
    "\n",
    "data.drop(data.loc[n].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59baa8",
   "metadata": {},
   "source": [
    "### Data trimmed because I literally couldn't load everything. 32gb ram not enough :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9888f70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>bit_rate</th>\n",
       "      <th>all_genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000002</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000003</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000005</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>000134</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>000136</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Rock]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>012381</td>\n",
       "      <td>320000</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002</th>\n",
       "      <td>012388</td>\n",
       "      <td>320000</td>\n",
       "      <td>[Electronic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7006</th>\n",
       "      <td>012392</td>\n",
       "      <td>320000</td>\n",
       "      <td>[Electronic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>012394</td>\n",
       "      <td>320000</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7010</th>\n",
       "      <td>012396</td>\n",
       "      <td>320000</td>\n",
       "      <td>[Electronic]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1673 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     track_id bit_rate    all_genres\n",
       "2      000002   256000     [Hip-Hop]\n",
       "3      000003   256000     [Hip-Hop]\n",
       "4      000005   256000     [Hip-Hop]\n",
       "11     000134   256000     [Hip-Hop]\n",
       "13     000136   256000        [Rock]\n",
       "...       ...      ...           ...\n",
       "6999   012381   320000     [Hip-Hop]\n",
       "7002   012388   320000  [Electronic]\n",
       "7006   012392   320000  [Electronic]\n",
       "7008   012394   320000     [Hip-Hop]\n",
       "7010   012396   320000  [Electronic]\n",
       "\n",
       "[1673 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fe47396",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = MultiLabelBinarizer()\n",
    "encgenres = pd.DataFrame(enc.fit_transform(data[\"all_genres\"]))\n",
    "classes = []\n",
    "\n",
    "for i in range(len(enc.classes_)):\n",
    "    classes.append([i,len(encgenres.loc[encgenres[i]==1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08886bbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rock: 782\n",
      "Folk: 241\n",
      "Punk: 214\n",
      "Hip-Hop: 195\n",
      "Electronic: 186\n",
      "Experimental: 157\n",
      "International: 112\n",
      "Indie-Rock: 109\n"
     ]
    }
   ],
   "source": [
    "for l in sorted(classes, key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{enc.classes_[l[0]]}: {l[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e186aa",
   "metadata": {},
   "source": [
    "### Big class imbalance, need to augment data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac0c732",
   "metadata": {},
   "source": [
    "First need to add audio data to the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "299a055a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_13092\\1865537184.py:7: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(sound_list[i], res_type='soxr_qq', sr=16000)\n",
      "C:\\Users\\miles\\school\\cs5850\\Genre Classification\\project\\env\\lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n"
     ]
    }
   ],
   "source": [
    "srs = []\n",
    "ys = []\n",
    "bad_files = []\n",
    "for i in range(len(sound_list)):\n",
    "    print(i)\n",
    "    try:\n",
    "        y, sr = librosa.load(sound_list[i], res_type='soxr_qq', sr=16000)\n",
    "    except NoBackendError as e:\n",
    "        bad_files.append(i)\n",
    "    ys.append(y)\n",
    "    srs.append(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6d1f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'track_id':result, 'y':ys, 'sr': srs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d0d2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94e33f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data = data.merge(df, on=\"track_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9de328ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make the original data OOO\n",
    "def rename_id(x):\n",
    "    return \"OOO-\"+x\n",
    "big_data.loc[:,\"track_id\"] = big_data[\"track_id\"].apply(rename_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aebbe13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>bit_rate</th>\n",
       "      <th>all_genres</th>\n",
       "      <th>y</th>\n",
       "      <th>sr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OOO-000002</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OOO-000003</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OOO-000005</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OOO-000134</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OOO-000136</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Rock]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>OOO-012381</td>\n",
       "      <td>320000</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>OOO-012388</td>\n",
       "      <td>320000</td>\n",
       "      <td>[Electronic]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>OOO-012392</td>\n",
       "      <td>320000</td>\n",
       "      <td>[Electronic]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>OOO-012394</td>\n",
       "      <td>320000</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1672</th>\n",
       "      <td>OOO-012396</td>\n",
       "      <td>320000</td>\n",
       "      <td>[Electronic]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1673 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        track_id bit_rate    all_genres   \n",
       "0     OOO-000002   256000     [Hip-Hop]  \\\n",
       "1     OOO-000003   256000     [Hip-Hop]   \n",
       "2     OOO-000005   256000     [Hip-Hop]   \n",
       "3     OOO-000134   256000     [Hip-Hop]   \n",
       "4     OOO-000136   256000        [Rock]   \n",
       "...          ...      ...           ...   \n",
       "1668  OOO-012381   320000     [Hip-Hop]   \n",
       "1669  OOO-012388   320000  [Electronic]   \n",
       "1670  OOO-012392   320000  [Electronic]   \n",
       "1671  OOO-012394   320000     [Hip-Hop]   \n",
       "1672  OOO-012396   320000  [Electronic]   \n",
       "\n",
       "                                                      y     sr  \n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  16000  \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  16000  \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  16000  \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  16000  \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  16000  \n",
       "...                                                 ...    ...  \n",
       "1668  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  16000  \n",
       "1669  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  16000  \n",
       "1670  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  16000  \n",
       "1671  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  16000  \n",
       "1672  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  16000  \n",
       "\n",
       "[1673 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e974c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentations\n",
    "\n",
    "def aug_noise(y, scale):\n",
    "    if scale > 0.5:\n",
    "        sys.exit(\"Scale too high on aug_noise\")\n",
    "    noise = np.random.normal(0, y.std(), y.size)\n",
    "    return y + noise*scale\n",
    "\n",
    "def aug_pitch(y, sr, scale):\n",
    "    if scale > 4 or scale < -4:\n",
    "        sys.exit(\"Scale out of bounds on aug_pitch\")\n",
    "    return librosa.effects.pitch_shift(y, sr=sr, n_steps=scale)\n",
    "\n",
    "def aug_polarity(y):\n",
    "    return y*-1\n",
    "\n",
    "def aug_gain(y, scale):\n",
    "    if scale > 5 or scale < 0.6:\n",
    "        sys.exit(\"Scale out of bounds on aug_pitch\")\n",
    "    return y * scale\n",
    "\n",
    "aug_list = [aug_polarity, aug_noise, aug_pitch, aug_gain]\n",
    "aug_num = [1, 5, 8, 9]\n",
    "\n",
    "\n",
    "# Augment\n",
    "def calc_augments(required_sets):\n",
    "    '''\n",
    "    Formula for number of resulting sets:\n",
    "    2**n + 16*i\n",
    "    where n is the number of types of augmentations\n",
    "    where i is the number of augmentations after the first 4 counting for n\n",
    "    It will always incremend n first, as that increases the number of datapoints the fastest\n",
    "    '''\n",
    "    augs_left = aug_num\n",
    "    transforms = []\n",
    "    total_sets = 1\n",
    "\n",
    "    for i in range(1,5):\n",
    "        transforms.append(aug_list[i-1])\n",
    "        if 2**i >= np.ceil(required_sets):\n",
    "            return transforms\n",
    "    required_sets-=16\n",
    "    \n",
    "    rs = required_sets / 16\n",
    "    pool = aug_list.copy()\n",
    "    pool.remove(aug_polarity)\n",
    "    for i in range(int(np.ceil(rs))):\n",
    "        cn = transforms.count(aug_noise)\n",
    "        cp = transforms.count(aug_pitch)\n",
    "        cg = transforms.count(aug_gain)\n",
    "        \n",
    "        if (cg < cp and cg <=8) or (cp==8 and cg <=8):\n",
    "            transforms.append(aug_list[3])\n",
    "        elif (cp < cn and cp <= 7) or (cn==5 and cp <=7):\n",
    "            transforms.append(aug_list[2])\n",
    "        elif cn <= 4:\n",
    "            transforms.append(aug_list[1])\n",
    "        else:\n",
    "            sys.exit(\"Not enough augmentation to reach target\")\n",
    "            \n",
    "    return transforms\n",
    "    \n",
    "\n",
    "def get_genre(x, genre):\n",
    "    return genre in x\n",
    "\n",
    "def get_args(transforms):\n",
    "    \n",
    "    transform_list = [0,0,0,0]\n",
    "    augments = []\n",
    "    for augment in transforms:\n",
    "        \n",
    "        if augment == aug_polarity:\n",
    "            \n",
    "            transform_list[0] += 1\n",
    "            if transform_list[0] == 2:\n",
    "                sys.exit(\"Too many aug_polarity\")\n",
    "            \n",
    "            augments.append([aug_polarity,[]])\n",
    "            \n",
    "        if augment == aug_noise:\n",
    "            \n",
    "            transform_list[1] += 1\n",
    "            if transform_list[1] == 6:\n",
    "                sys.exit(\"Too many aug_noise\")\n",
    "            \n",
    "            augments.append([aug_noise, [0.1*transform_list[1]]])\n",
    "        \n",
    "        if augment == aug_pitch:\n",
    "            \n",
    "            transform_list[2] += 1\n",
    "            if transform_list[2] == 9:\n",
    "                sys.exit(\"Too many aug_pitch\")\n",
    "            \n",
    "            augments.append([aug_pitch, [(-1)**(transform_list[2]+1)*np.ceil(transform_list[2]/2)]])\n",
    "        \n",
    "        if augment == aug_gain:\n",
    "            \n",
    "            transform_list[3] += 1\n",
    "            if transform_list[3] == 10:\n",
    "                sys.exit(\"Too many aug_gain\")\n",
    "            \n",
    "            if transform_list[3] % 2 == 1:\n",
    "                augments.append([aug_gain, [(0.5*transform_list[3])+1.5]])\n",
    "            else:\n",
    "                augments.append([aug_gain, [((-1/20)*transform_list[3])+1]])\n",
    "    \n",
    "    return augments\n",
    "\n",
    "def apply_augmentations(augs, ind):\n",
    "    for i, aug in enumerate(augs):\n",
    "        if index == []:\n",
    "            index.append(str(0).zfill(3)+ind.track_id[3:])\n",
    "        else:\n",
    "            index.append(str(int(index[-1][:3])+1).zfill(3)+ind.track_id[3:])\n",
    "        if aug[0] == aug_pitch:\n",
    "            augmented.append(aug[0](ind.y,ind.sr, *aug[1]))\n",
    "        else:\n",
    "            augmented.append(aug[0](ind.y, *aug[1]))\n",
    "        if i < len(augs) -1:\n",
    "            apply_augmentations(augs[i+1:], ind)\n",
    "    \n",
    "\n",
    "def augment_data(genre, target):\n",
    "    gs = big_data.loc[big_data[\"all_genres\"].apply(get_genre, args=(genre,))]\n",
    "    current = len(gs)\n",
    "    required_sets = target/current\n",
    "    \n",
    "    transforms = calc_augments(required_sets)\n",
    "    augments = get_args(transforms)\n",
    "    \n",
    "    big_test = pd.DataFrame()\n",
    "    for i in range(len(gs)):\n",
    "        \n",
    "        if current+len(big_test) >= target:\n",
    "            return big_test\n",
    "        \n",
    "        print(f\"{genre}: {(current+len(big_test))/target}\")\n",
    "        apply_augmentations(augments, gs.iloc[i])\n",
    "    \n",
    "    \n",
    "        dic = {\"track_id\":index,\"y\":augmented}\n",
    "        df = pd.DataFrame(dic)\n",
    "        \n",
    "        df[\"bit_rate\"] = gs.iloc[i].bit_rate\n",
    "        df[\"all_genres\"] = 0\n",
    "        df[\"all_genres\"] = [gs.iloc[i].all_genres] *len(df)\n",
    "        df[\"sr\"] = gs.iloc[i].sr\n",
    "        \n",
    "        big_test = pd.concat([big_test,df], ignore_index=True)\n",
    "        \n",
    "    return big_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e017ec",
   "metadata": {},
   "source": [
    "### Augment the Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7691439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indie-Rock: 0.218\n",
      "Indie-Rock: 0.232\n",
      "Indie-Rock: 0.26\n",
      "Indie-Rock: 0.302\n",
      "Indie-Rock: 0.358\n",
      "Indie-Rock: 0.428\n",
      "Indie-Rock: 0.512\n",
      "Indie-Rock: 0.61\n",
      "Indie-Rock: 0.722\n",
      "Indie-Rock: 0.848\n",
      "Indie-Rock: 0.988\n",
      "International: 0.224\n",
      "International: 0.238\n",
      "International: 0.266\n",
      "International: 0.308\n",
      "International: 0.364\n",
      "International: 0.434\n",
      "International: 0.518\n",
      "International: 0.616\n",
      "International: 0.728\n",
      "International: 0.854\n",
      "International: 0.994\n",
      "Experimental: 0.314\n",
      "Experimental: 0.32\n",
      "Experimental: 0.332\n",
      "Experimental: 0.35\n",
      "Experimental: 0.374\n",
      "Experimental: 0.404\n",
      "Experimental: 0.44\n",
      "Experimental: 0.482\n",
      "Experimental: 0.53\n",
      "Experimental: 0.584\n",
      "Experimental: 0.644\n",
      "Experimental: 0.71\n",
      "Experimental: 0.782\n",
      "Experimental: 0.86\n",
      "Experimental: 0.944\n",
      "Electronic: 0.372\n",
      "Electronic: 0.378\n",
      "Electronic: 0.39\n",
      "Electronic: 0.408\n",
      "Electronic: 0.432\n",
      "Electronic: 0.462\n",
      "Electronic: 0.498\n",
      "Electronic: 0.54\n",
      "Electronic: 0.588\n",
      "Electronic: 0.642\n",
      "Electronic: 0.702\n",
      "Electronic: 0.768\n",
      "Electronic: 0.84\n",
      "Electronic: 0.918\n",
      "Hip-Hop: 0.39\n",
      "Hip-Hop: 0.396\n",
      "Hip-Hop: 0.408\n",
      "Hip-Hop: 0.426\n",
      "Hip-Hop: 0.45\n",
      "Hip-Hop: 0.48\n",
      "Hip-Hop: 0.516\n",
      "Hip-Hop: 0.558\n",
      "Hip-Hop: 0.606\n",
      "Hip-Hop: 0.66\n",
      "Hip-Hop: 0.72\n",
      "Hip-Hop: 0.786\n",
      "Hip-Hop: 0.858\n",
      "Hip-Hop: 0.936\n",
      "Punk: 0.428\n",
      "Punk: 0.434\n",
      "Punk: 0.446\n",
      "Punk: 0.464\n",
      "Punk: 0.488\n",
      "Punk: 0.518\n",
      "Punk: 0.554\n",
      "Punk: 0.596\n",
      "Punk: 0.644\n",
      "Punk: 0.698\n",
      "Punk: 0.758\n",
      "Punk: 0.824\n",
      "Punk: 0.896\n",
      "Punk: 0.974\n",
      "Folk: 0.482\n",
      "Folk: 0.488\n",
      "Folk: 0.5\n",
      "Folk: 0.518\n",
      "Folk: 0.542\n",
      "Folk: 0.572\n",
      "Folk: 0.608\n",
      "Folk: 0.65\n",
      "Folk: 0.698\n",
      "Folk: 0.752\n",
      "Folk: 0.812\n",
      "Folk: 0.878\n",
      "Folk: 0.95\n"
     ]
    }
   ],
   "source": [
    "enc = MultiLabelBinarizer()\n",
    "encgenres = pd.DataFrame(enc.fit_transform(big_data[\"all_genres\"]))\n",
    "classes = []\n",
    "for i in range(len(enc.classes_)):\n",
    "    classes.append([i,len(encgenres.loc[encgenres[i]==1])])\n",
    "    \n",
    "for genre in sorted(classes, key=lambda x: x[1]):\n",
    "    \n",
    "    if l[1] < 500:\n",
    "        augmented = []\n",
    "        index = []\n",
    "        df = augment_data(enc.classes_[genre[0]], 500)\n",
    "        big_data = pd.concat([big_data, df], ignore_index=True)\n",
    "        encgenres = pd.DataFrame(enc.fit_transform(big_data[\"all_genres\"]))\n",
    "        classes = []\n",
    "        for i in range(len(enc.classes_)):\n",
    "            classes.append([i,len(encgenres.loc[encgenres[i]==1])])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c901ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>bit_rate</th>\n",
       "      <th>all_genres</th>\n",
       "      <th>y</th>\n",
       "      <th>sr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OOO-000002</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OOO-000003</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OOO-000005</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OOO-000134</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OOO-000136</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Rock]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4170</th>\n",
       "      <td>034-000207</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Folk]</td>\n",
       "      <td>[0.0039603878492535894, 0.005395981476878981, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4171</th>\n",
       "      <td>035-000207</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Folk]</td>\n",
       "      <td>[-0.015787465626385718, -0.0016790618280483176...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>036-000208</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Folk]</td>\n",
       "      <td>[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0....</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>037-000208</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Folk]</td>\n",
       "      <td>[0.002946226188674887, 0.013041241494400349, 0...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>038-000208</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Folk]</td>\n",
       "      <td>[0.005701005566428094, -0.012420363482642673, ...</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4175 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        track_id bit_rate all_genres   \n",
       "0     OOO-000002   256000  [Hip-Hop]  \\\n",
       "1     OOO-000003   256000  [Hip-Hop]   \n",
       "2     OOO-000005   256000  [Hip-Hop]   \n",
       "3     OOO-000134   256000  [Hip-Hop]   \n",
       "4     OOO-000136   256000     [Rock]   \n",
       "...          ...      ...        ...   \n",
       "4170  034-000207   256000     [Folk]   \n",
       "4171  035-000207   256000     [Folk]   \n",
       "4172  036-000208   256000     [Folk]   \n",
       "4173  037-000208   256000     [Folk]   \n",
       "4174  038-000208   256000     [Folk]   \n",
       "\n",
       "                                                      y     sr  \n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  16000  \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  16000  \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  16000  \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  16000  \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  16000  \n",
       "...                                                 ...    ...  \n",
       "4170  [0.0039603878492535894, 0.005395981476878981, ...  16000  \n",
       "4171  [-0.015787465626385718, -0.0016790618280483176...  16000  \n",
       "4172  [-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0....  16000  \n",
       "4173  [0.002946226188674887, 0.013041241494400349, 0...  16000  \n",
       "4174  [0.005701005566428094, -0.012420363482642673, ...  16000  \n",
       "\n",
       "[4175 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3684f755",
   "metadata": {},
   "source": [
    "### Much better balance. Rock is still above the rest, but not by as much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f96a6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rock: 1559\n",
      "International: 574\n",
      "Indie-Rock: 571\n",
      "Punk: 529\n",
      "Experimental: 517\n",
      "Folk: 514\n",
      "Hip-Hop: 510\n",
      "Electronic: 501\n"
     ]
    }
   ],
   "source": [
    "enc = MultiLabelBinarizer()\n",
    "encgenres = pd.DataFrame(enc.fit_transform(big_data[\"all_genres\"]))\n",
    "classes = []\n",
    "\n",
    "for i in range(len(enc.classes_)):\n",
    "    classes.append([i,len(encgenres.loc[encgenres[i]==1])])\n",
    "\n",
    "for l in sorted(classes, key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{enc.classes_[l[0]]}: {l[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd8f7f",
   "metadata": {},
   "source": [
    "### Time to extract info from the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "id": "858bbcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_length = 1024\n",
    "big_data[\"mfcc\"] = big_data.apply(lambda row: librosa.feature.mfcc(y=row[\"y\"], sr=row[\"sr\"], hop_length=hop_length, n_mfcc=10), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "id": "b86740bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data[\"chroma\"] = big_data.apply(lambda row: librosa.feature.chroma_stft(y=row[\"y\"], sr=row[\"sr\"],n_chroma=10, hop_length=hop_length,n_fft=2048), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "id": "8770b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data[\"spectral\"] = big_data.apply(lambda row: librosa.feature.spectral_centroid(y=row[\"y\"], sr=row[\"sr\"],hop_length=hop_length),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "id": "5b47c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data[\"zcross\"] = big_data.apply(lambda row: librosa.feature.zero_crossing_rate(row[\"y\"],hop_length=hop_length),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69acf7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1291ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = enc.fit_transform(big_data[\"all_genres\"])\n",
    "encgenres = pd.DataFrame(encoded, columns=enc.classes_)\n",
    "big_data = pd.concat([big_data, encgenres], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daebb20c",
   "metadata": {},
   "source": [
    "### Final dataframe in all it's glory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "id": "47adb755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>bit_rate</th>\n",
       "      <th>all_genres</th>\n",
       "      <th>y</th>\n",
       "      <th>sr</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>chroma</th>\n",
       "      <th>spectral</th>\n",
       "      <th>zcross</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Experimental</th>\n",
       "      <th>Folk</th>\n",
       "      <th>Hip-Hop</th>\n",
       "      <th>Indie-Rock</th>\n",
       "      <th>International</th>\n",
       "      <th>Punk</th>\n",
       "      <th>Rock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>012-000369</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Punk, Rock]</td>\n",
       "      <td>[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0....</td>\n",
       "      <td>16000</td>\n",
       "      <td>[[-261.75668, -135.58673, -15.239572, -12.3259...</td>\n",
       "      <td>[[0.26851124, 0.04189693, 0.78985626, 0.161644...</td>\n",
       "      <td>[[1481.6807778740895, 1708.9312617305268, 1057...</td>\n",
       "      <td>[[0.01513671875, 0.0400390625, 0.0498046875, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>003-000824</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Indie-Rock, Rock]</td>\n",
       "      <td>[-0.00035237466, -0.00041079277, -0.0004094517...</td>\n",
       "      <td>16000</td>\n",
       "      <td>[[-77.59873, 50.99259, 51.84704, 30.608698, 36...</td>\n",
       "      <td>[[0.31460342, 0.45461187, 0.45708284, 0.434485...</td>\n",
       "      <td>[[1594.1823829111877, 1701.5023615218697, 1888...</td>\n",
       "      <td>[[0.068359375, 0.1494140625, 0.158203125, 0.12...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922</th>\n",
       "      <td>002-000139</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Folk]</td>\n",
       "      <td>[-0.0029076350251451165, -0.000255725846660055...</td>\n",
       "      <td>16000</td>\n",
       "      <td>[[-232.60339620083997, -114.0510718783331, -10...</td>\n",
       "      <td>[[1.0, 0.03371930248921651, 0.0307432699198376...</td>\n",
       "      <td>[[3885.57570037974, 2373.8713084679302, 2922.8...</td>\n",
       "      <td>[[0.185546875, 0.25732421875, 0.1484375, 0.160...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>033-000830</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Indie-Rock, Rock]</td>\n",
       "      <td>[-2.7789813e-05, -9.041599e-05, 3.7759415e-05,...</td>\n",
       "      <td>16000</td>\n",
       "      <td>[[-82.51235, 42.72989, 15.14737, 27.91318, 40....</td>\n",
       "      <td>[[0.88533676, 0.5499044, 0.19738185, 0.2592471...</td>\n",
       "      <td>[[1229.034387857203, 1430.1723981548362, 1511....</td>\n",
       "      <td>[[0.02490234375, 0.0693359375, 0.078125, 0.078...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3693</th>\n",
       "      <td>022-000896</td>\n",
       "      <td>160000</td>\n",
       "      <td>[Punk, Rock]</td>\n",
       "      <td>[0.016127849910157782, -0.00218808204209632, -...</td>\n",
       "      <td>16000</td>\n",
       "      <td>[[-106.94361725707908, 43.24532644570139, 61.8...</td>\n",
       "      <td>[[1.0, 1.0, 0.9583717596070912, 1.0, 1.0, 1.0,...</td>\n",
       "      <td>[[2679.8670802710053, 2087.4985474172277, 2298...</td>\n",
       "      <td>[[0.13916015625, 0.22998046875, 0.21044921875,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>005-000824</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Indie-Rock, Rock]</td>\n",
       "      <td>[-0.00035237466, -0.00041079277, -0.0004094517...</td>\n",
       "      <td>16000</td>\n",
       "      <td>[[-77.59873, 50.99259, 51.84704, 30.608698, 36...</td>\n",
       "      <td>[[0.31460342, 0.45461187, 0.45708284, 0.434485...</td>\n",
       "      <td>[[1594.1823829111877, 1701.5023615218697, 1888...</td>\n",
       "      <td>[[0.068359375, 0.1494140625, 0.158203125, 0.12...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>038-003401</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Indie-Rock, Rock]</td>\n",
       "      <td>[-4.1383824e-05, 1.6528917e-05, 2.2222854e-05,...</td>\n",
       "      <td>16000</td>\n",
       "      <td>[[-214.4505, -82.32879, -68.93448, -114.964745...</td>\n",
       "      <td>[[0.4040403, 0.05229712, 0.08758459, 0.1518849...</td>\n",
       "      <td>[[1995.9936154387156, 2064.2353231500338, 2171...</td>\n",
       "      <td>[[0.0625, 0.126953125, 0.1611328125, 0.1933593...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>000-000148</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Experimental]</td>\n",
       "      <td>[-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0....</td>\n",
       "      <td>16000</td>\n",
       "      <td>[[-148.78394, -2.6720407, 4.8523464, 22.647324...</td>\n",
       "      <td>[[0.18146876, 0.23321348, 0.10517499, 0.095677...</td>\n",
       "      <td>[[921.5792350792062, 1081.1882811801433, 1180....</td>\n",
       "      <td>[[0.02197265625, 0.0673828125, 0.0869140625, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>060-000853</td>\n",
       "      <td>256000</td>\n",
       "      <td>[International]</td>\n",
       "      <td>[-0.011478485780449998, -0.01089870431992549, ...</td>\n",
       "      <td>16000</td>\n",
       "      <td>[[-143.55179495411434, -65.95086143357136, -72...</td>\n",
       "      <td>[[0.3280126473277786, 0.11603360777912516, 0.2...</td>\n",
       "      <td>[[2330.5506054735656, 1756.303577730089, 1962....</td>\n",
       "      <td>[[0.115234375, 0.15673828125, 0.08642578125, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>OOO-011412</td>\n",
       "      <td>256000</td>\n",
       "      <td>[Indie-Rock, Rock]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>16000</td>\n",
       "      <td>[[-155.09001, -63.38529, -92.92886, -16.261826...</td>\n",
       "      <td>[[0.9009251, 0.9332691, 0.32482573, 0.8498876,...</td>\n",
       "      <td>[[1960.194028084242, 1799.0733817715777, 2353....</td>\n",
       "      <td>[[0.05078125, 0.1015625, 0.1484375, 0.16601562...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4175 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        track_id bit_rate          all_genres   \n",
       "3797  012-000369   256000        [Punk, Rock]  \\\n",
       "1781  003-000824   256000  [Indie-Rock, Rock]   \n",
       "3922  002-000139   256000              [Folk]   \n",
       "2021  033-000830   256000  [Indie-Rock, Rock]   \n",
       "3693  022-000896   160000        [Punk, Rock]   \n",
       "...          ...      ...                 ...   \n",
       "1825  005-000824   256000  [Indie-Rock, Rock]   \n",
       "2096  038-003401   256000  [Indie-Rock, Rock]   \n",
       "2705  000-000148   256000      [Experimental]   \n",
       "2510  060-000853   256000     [International]   \n",
       "1446  OOO-011412   256000  [Indie-Rock, Rock]   \n",
       "\n",
       "                                                      y     sr   \n",
       "3797  [-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0....  16000  \\\n",
       "1781  [-0.00035237466, -0.00041079277, -0.0004094517...  16000   \n",
       "3922  [-0.0029076350251451165, -0.000255725846660055...  16000   \n",
       "2021  [-2.7789813e-05, -9.041599e-05, 3.7759415e-05,...  16000   \n",
       "3693  [0.016127849910157782, -0.00218808204209632, -...  16000   \n",
       "...                                                 ...    ...   \n",
       "1825  [-0.00035237466, -0.00041079277, -0.0004094517...  16000   \n",
       "2096  [-4.1383824e-05, 1.6528917e-05, 2.2222854e-05,...  16000   \n",
       "2705  [-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0....  16000   \n",
       "2510  [-0.011478485780449998, -0.01089870431992549, ...  16000   \n",
       "1446  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  16000   \n",
       "\n",
       "                                                   mfcc   \n",
       "3797  [[-261.75668, -135.58673, -15.239572, -12.3259...  \\\n",
       "1781  [[-77.59873, 50.99259, 51.84704, 30.608698, 36...   \n",
       "3922  [[-232.60339620083997, -114.0510718783331, -10...   \n",
       "2021  [[-82.51235, 42.72989, 15.14737, 27.91318, 40....   \n",
       "3693  [[-106.94361725707908, 43.24532644570139, 61.8...   \n",
       "...                                                 ...   \n",
       "1825  [[-77.59873, 50.99259, 51.84704, 30.608698, 36...   \n",
       "2096  [[-214.4505, -82.32879, -68.93448, -114.964745...   \n",
       "2705  [[-148.78394, -2.6720407, 4.8523464, 22.647324...   \n",
       "2510  [[-143.55179495411434, -65.95086143357136, -72...   \n",
       "1446  [[-155.09001, -63.38529, -92.92886, -16.261826...   \n",
       "\n",
       "                                                 chroma   \n",
       "3797  [[0.26851124, 0.04189693, 0.78985626, 0.161644...  \\\n",
       "1781  [[0.31460342, 0.45461187, 0.45708284, 0.434485...   \n",
       "3922  [[1.0, 0.03371930248921651, 0.0307432699198376...   \n",
       "2021  [[0.88533676, 0.5499044, 0.19738185, 0.2592471...   \n",
       "3693  [[1.0, 1.0, 0.9583717596070912, 1.0, 1.0, 1.0,...   \n",
       "...                                                 ...   \n",
       "1825  [[0.31460342, 0.45461187, 0.45708284, 0.434485...   \n",
       "2096  [[0.4040403, 0.05229712, 0.08758459, 0.1518849...   \n",
       "2705  [[0.18146876, 0.23321348, 0.10517499, 0.095677...   \n",
       "2510  [[0.3280126473277786, 0.11603360777912516, 0.2...   \n",
       "1446  [[0.9009251, 0.9332691, 0.32482573, 0.8498876,...   \n",
       "\n",
       "                                               spectral   \n",
       "3797  [[1481.6807778740895, 1708.9312617305268, 1057...  \\\n",
       "1781  [[1594.1823829111877, 1701.5023615218697, 1888...   \n",
       "3922  [[3885.57570037974, 2373.8713084679302, 2922.8...   \n",
       "2021  [[1229.034387857203, 1430.1723981548362, 1511....   \n",
       "3693  [[2679.8670802710053, 2087.4985474172277, 2298...   \n",
       "...                                                 ...   \n",
       "1825  [[1594.1823829111877, 1701.5023615218697, 1888...   \n",
       "2096  [[1995.9936154387156, 2064.2353231500338, 2171...   \n",
       "2705  [[921.5792350792062, 1081.1882811801433, 1180....   \n",
       "2510  [[2330.5506054735656, 1756.303577730089, 1962....   \n",
       "1446  [[1960.194028084242, 1799.0733817715777, 2353....   \n",
       "\n",
       "                                                 zcross  Electronic   \n",
       "3797  [[0.01513671875, 0.0400390625, 0.0498046875, 0...           0  \\\n",
       "1781  [[0.068359375, 0.1494140625, 0.158203125, 0.12...           0   \n",
       "3922  [[0.185546875, 0.25732421875, 0.1484375, 0.160...           0   \n",
       "2021  [[0.02490234375, 0.0693359375, 0.078125, 0.078...           0   \n",
       "3693  [[0.13916015625, 0.22998046875, 0.21044921875,...           0   \n",
       "...                                                 ...         ...   \n",
       "1825  [[0.068359375, 0.1494140625, 0.158203125, 0.12...           0   \n",
       "2096  [[0.0625, 0.126953125, 0.1611328125, 0.1933593...           0   \n",
       "2705  [[0.02197265625, 0.0673828125, 0.0869140625, 0...           0   \n",
       "2510  [[0.115234375, 0.15673828125, 0.08642578125, 0...           0   \n",
       "1446  [[0.05078125, 0.1015625, 0.1484375, 0.16601562...           0   \n",
       "\n",
       "      Experimental  Folk  Hip-Hop  Indie-Rock  International  Punk  Rock  \n",
       "3797             0     0        0           0              0     1     1  \n",
       "1781             0     0        0           1              0     0     1  \n",
       "3922             0     1        0           0              0     0     0  \n",
       "2021             0     0        0           1              0     0     1  \n",
       "3693             0     0        0           0              0     1     1  \n",
       "...            ...   ...      ...         ...            ...   ...   ...  \n",
       "1825             0     0        0           1              0     0     1  \n",
       "2096             0     0        0           1              0     0     1  \n",
       "2705             1     0        0           0              0     0     0  \n",
       "2510             0     0        0           0              1     0     0  \n",
       "1446             0     0        0           1              0     0     1  \n",
       "\n",
       "[4175 rows x 17 columns]"
      ]
     },
     "execution_count": 1028,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0bbff6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b410865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bc3ce50",
   "metadata": {},
   "source": [
    "### Get it ready for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e49ff555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c884ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data = big_data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "id": "eae974e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = big_data[[\"Electronic\", \"Experimental\", \"Folk\", \"Hip-Hop\", \"International\", \"Punk\", \"Rock\"]]\n",
    "X = big_data[[\"bit_rate\", \"y\", \"mfcc\", \"chroma\", \"spectral\"]]\n",
    "X.loc[:,'mfcc'] = X['mfcc'].apply(lambda x: x.reshape(-1,10,469,1))\n",
    "mfcc = np.array(X['mfcc'].tolist())\n",
    "trainX, testX, trainY, testY = train_test_split(mfcc, Y, stratify=Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "48b1a89c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "b52a6714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4175, 1, 10, 469, 1)"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a10326",
   "metadata": {},
   "source": [
    "### Dummy Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96e7fa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22954584367956749\n",
      "0.15923486239750526\n",
      "0.4929623209236055\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.54      0.21       100\n",
      "           1       0.11      0.45      0.18       103\n",
      "           2       0.13      0.54      0.21       103\n",
      "           3       0.13      0.50      0.20       102\n",
      "           4       0.14      0.52      0.22       115\n",
      "           5       0.11      0.43      0.18       106\n",
      "           6       0.37      0.46      0.41       312\n",
      "\n",
      "   micro avg       0.16      0.49      0.24       941\n",
      "   macro avg       0.16      0.49      0.23       941\n",
      "weighted avg       0.20      0.49      0.27       941\n",
      " samples avg       0.15      0.49      0.23       941\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\school\\cs5850\\Genre Classification\\project\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"uniform\")\n",
    "dummy_clf.fit(trainX, trainY)\n",
    "preds = dummy_clf.predict(testX)\n",
    "\n",
    "print(f1_score(testY, preds, average=\"macro\"))\n",
    "print(precision_score(testY, preds, average=\"macro\"))\n",
    "print(recall_score(testY, preds, average=\"macro\"))\n",
    "print(classification_report(testY, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d667430a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.182679703403005\n",
      "0.18281176272157396\n",
      "0.18321875544594407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.14      0.15       100\n",
      "           1       0.16      0.17      0.17       103\n",
      "           2       0.11      0.12      0.11       103\n",
      "           3       0.14      0.16      0.15       102\n",
      "           4       0.08      0.10      0.09       115\n",
      "           5       0.21      0.20      0.20       106\n",
      "           6       0.41      0.40      0.40       312\n",
      "\n",
      "   micro avg       0.23      0.23      0.23       941\n",
      "   macro avg       0.18      0.18      0.18       941\n",
      "weighted avg       0.23      0.23      0.23       941\n",
      " samples avg       0.17      0.22      0.18       941\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\school\\cs5850\\Genre Classification\\project\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "dummy_clf.fit(trainX, trainY)\n",
    "preds = dummy_clf.predict(testX)\n",
    "\n",
    "print(f1_score(testY, preds, average=\"macro\"))\n",
    "print(precision_score(testY, preds, average=\"macro\"))\n",
    "print(recall_score(testY, preds, average=\"macro\"))\n",
    "print(classification_report(testY, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd2bfaa",
   "metadata": {},
   "source": [
    "### Comparing Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "id": "424dd6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "53/53 [==============================] - 1s 11ms/step - loss: 9.3140 - accuracy: 0.1308 - val_loss: 9.5763 - val_accuracy: 0.1329\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 10.8180 - accuracy: 0.1368 - val_loss: 11.5260 - val_accuracy: 0.1581\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 9.6560 - accuracy: 0.1524 - val_loss: 8.5146 - val_accuracy: 0.1689\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 8.9135 - accuracy: 0.1530 - val_loss: 10.5009 - val_accuracy: 0.1892\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 9.9134 - accuracy: 0.1632 - val_loss: 9.3813 - val_accuracy: 0.1868\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 9.6420 - accuracy: 0.1581 - val_loss: 10.7906 - val_accuracy: 0.1880\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 10.0335 - accuracy: 0.1719 - val_loss: 10.4044 - val_accuracy: 0.1832\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 9.5938 - accuracy: 0.1635 - val_loss: 10.1534 - val_accuracy: 0.1904\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 9.5651 - accuracy: 0.1713 - val_loss: 8.9180 - val_accuracy: 0.1904\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 9.3139 - accuracy: 0.1623 - val_loss: 8.6092 - val_accuracy: 0.1868\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 8.6092 - accuracy: 0.1868\n",
      "[test loss, test accuracy]: [8.609187126159668, 0.18682634830474854]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(1, 10, 469, 1)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(7)\n",
    "])\n",
    "model.compile(loss=\"CategoricalCrossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=10, batch_size=64)\n",
    "eval_result = model.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "id": "13f4176c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 33.9662 - accuracy: 0.3716 - val_loss: 2.1518 - val_accuracy: 0.3198\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.2159 - accuracy: 0.3362 - val_loss: 2.2842 - val_accuracy: 0.3138\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.1838 - accuracy: 0.3446 - val_loss: 2.3453 - val_accuracy: 0.3377\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.0098 - accuracy: 0.3521 - val_loss: 1.9797 - val_accuracy: 0.3808\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.8682 - accuracy: 0.3557 - val_loss: 3.7068 - val_accuracy: 0.3365\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.8783 - accuracy: 0.3719 - val_loss: 2.0879 - val_accuracy: 0.3749\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.7535 - accuracy: 0.3647 - val_loss: 2.1062 - val_accuracy: 0.3425\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.7530 - accuracy: 0.3725 - val_loss: 1.6467 - val_accuracy: 0.3832\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.7346 - accuracy: 0.3784 - val_loss: 2.6081 - val_accuracy: 0.3569\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.7558 - accuracy: 0.3656 - val_loss: 2.5206 - val_accuracy: 0.3749\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 2.5206 - accuracy: 0.3749\n",
      "[test loss, test accuracy]: [2.520573854446411, 0.3748503029346466]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(1, 10, 469, 1)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(7)\n",
    "])\n",
    "model.compile(loss=\"CategoricalHinge\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=10, batch_size=64)\n",
    "eval_result = model.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "id": "effedc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 6.0048 - accuracy: 0.4006 - val_loss: 0.9793 - val_accuracy: 0.3820\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.8316 - accuracy: 0.3868 - val_loss: 0.8557 - val_accuracy: 0.4132\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.7354 - accuracy: 0.4141 - val_loss: 0.7991 - val_accuracy: 0.4335\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.6237 - accuracy: 0.4491 - val_loss: 0.6860 - val_accuracy: 0.4683\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5341 - accuracy: 0.4614 - val_loss: 0.6682 - val_accuracy: 0.4982\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5010 - accuracy: 0.4554 - val_loss: 0.5397 - val_accuracy: 0.4766\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.4453 - accuracy: 0.4790 - val_loss: 0.5627 - val_accuracy: 0.4719\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.4219 - accuracy: 0.4698 - val_loss: 0.4980 - val_accuracy: 0.4994\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.3856 - accuracy: 0.4674 - val_loss: 0.5396 - val_accuracy: 0.4659\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.3437 - accuracy: 0.4467 - val_loss: 0.4481 - val_accuracy: 0.4754\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.4481 - accuracy: 0.4754\n",
      "[test loss, test accuracy]: [0.4480886161327362, 0.4754491150379181]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(1, 10, 469, 1)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(7)\n",
    "])\n",
    "model.compile(loss=\"Hinge\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=10, batch_size=64)\n",
    "eval_result = model.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "id": "d4f45f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 319.1454 - accuracy: 0.2892 - val_loss: 14.7599 - val_accuracy: 0.3006\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7.4226 - accuracy: 0.3045 - val_loss: 23.4602 - val_accuracy: 0.3377\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8.9615 - accuracy: 0.3171 - val_loss: 17.9047 - val_accuracy: 0.3246\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.7673 - accuracy: 0.3219 - val_loss: 19.4199 - val_accuracy: 0.3281\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.7914 - accuracy: 0.3380 - val_loss: 12.2841 - val_accuracy: 0.3293\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.5304 - accuracy: 0.3410 - val_loss: 23.7178 - val_accuracy: 0.3257\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.1549 - accuracy: 0.3374 - val_loss: 16.4001 - val_accuracy: 0.3377\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.0377 - accuracy: 0.3362 - val_loss: 29.1351 - val_accuracy: 0.3293\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.3686 - accuracy: 0.3422 - val_loss: 24.6665 - val_accuracy: 0.3257\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.0990 - accuracy: 0.3347 - val_loss: 21.2117 - val_accuracy: 0.3257\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 21.2117 - accuracy: 0.3257\n",
      "[test loss, test accuracy]: [21.21174430847168, 0.3257485032081604]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(1, 10, 469, 1)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(7)\n",
    "])\n",
    "model.compile(loss=\"SquaredHinge\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=10, batch_size=64)\n",
    "\n",
    "eval_result = model.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f80775",
   "metadata": {},
   "source": [
    "### Tuned FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "id": "ccd93625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(1, 10, 469, 1)))\n",
    "    \n",
    "    hp_units = hp.Int('units', min_value=32, max_value=128, step=32)\n",
    "    hp_units1 = hp.Int('units1', min_value=64, max_value=256, step=64)\n",
    "    hp_units2 = hp.Int('units2', min_value=64, max_value=256, step=64)\n",
    "    hp_units3 = hp.Int('units3', min_value=64, max_value=256, step=64)\n",
    "    hp_units4 = hp.Int('units4', min_value=64, max_value=256, step=64)\n",
    "    \n",
    "    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "    model.add(keras.layers.Dense(units=hp_units1, activation='relu'))\n",
    "    model.add(keras.layers.Dense(units=hp_units2, activation='relu'))\n",
    "    model.add(keras.layers.Dense(units=hp_units3, activation='relu'))\n",
    "    model.add(keras.layers.Dense(units=hp_units4, activation='relu'))\n",
    "    \n",
    "    hp_drop = hp.Float('drop', min_value=0, max_value=0.5, step=0.1)\n",
    "    \n",
    "    model.add(tf.keras.layers.Dropout(rate=hp_drop),)\n",
    "    \n",
    "    model.add(keras.layers.Dense(7))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.CategoricalHinge(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "id": "6fb9b569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3)\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(trainX, trainY, epochs=100, validation_split=0.2, callbacks=[stop_early])\n",
    "fcbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "id": "db463209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0232 - accuracy: 0.2975 - val_loss: 0.9702 - val_accuracy: 0.4207\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.9275 - accuracy: 0.4615 - val_loss: 0.8072 - val_accuracy: 0.5479\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.7574 - accuracy: 0.5696 - val_loss: 0.6075 - val_accuracy: 0.6198\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.5540 - accuracy: 0.6490 - val_loss: 0.4772 - val_accuracy: 0.6407\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4527 - accuracy: 0.6677 - val_loss: 0.4552 - val_accuracy: 0.6392\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3965 - accuracy: 0.6972 - val_loss: 0.4324 - val_accuracy: 0.6512\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3573 - accuracy: 0.7107 - val_loss: 0.4180 - val_accuracy: 0.6632\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3200 - accuracy: 0.7324 - val_loss: 0.4260 - val_accuracy: 0.6557\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2811 - accuracy: 0.7485 - val_loss: 0.4121 - val_accuracy: 0.6737\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2548 - accuracy: 0.7672 - val_loss: 0.4289 - val_accuracy: 0.6722\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2328 - accuracy: 0.7743 - val_loss: 0.4698 - val_accuracy: 0.6722\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2135 - accuracy: 0.7844 - val_loss: 0.4264 - val_accuracy: 0.6617\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1865 - accuracy: 0.7945 - val_loss: 0.5031 - val_accuracy: 0.6213\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1602 - accuracy: 0.8091 - val_loss: 0.4512 - val_accuracy: 0.6602\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1387 - accuracy: 0.8192 - val_loss: 0.4765 - val_accuracy: 0.6542\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1484 - accuracy: 0.8196 - val_loss: 0.5438 - val_accuracy: 0.6228\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1357 - accuracy: 0.8211 - val_loss: 0.4641 - val_accuracy: 0.6572\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1032 - accuracy: 0.8350 - val_loss: 0.5070 - val_accuracy: 0.6437\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0825 - accuracy: 0.8466 - val_loss: 0.4924 - val_accuracy: 0.6587\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0838 - accuracy: 0.8458 - val_loss: 0.5450 - val_accuracy: 0.6437\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0619 - accuracy: 0.8544 - val_loss: 0.5403 - val_accuracy: 0.6422\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0475 - accuracy: 0.8623 - val_loss: 0.5246 - val_accuracy: 0.6572\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0386 - accuracy: 0.8623 - val_loss: 0.5453 - val_accuracy: 0.6766\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0430 - accuracy: 0.8615 - val_loss: 0.5571 - val_accuracy: 0.6392\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0291 - accuracy: 0.8675 - val_loss: 0.5573 - val_accuracy: 0.6647\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0362 - accuracy: 0.8664 - val_loss: 0.5976 - val_accuracy: 0.6677\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0663 - accuracy: 0.8510 - val_loss: 0.5718 - val_accuracy: 0.6482\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0358 - accuracy: 0.8668 - val_loss: 0.6359 - val_accuracy: 0.6123\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0202 - accuracy: 0.8713 - val_loss: 0.5544 - val_accuracy: 0.6587\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0130 - accuracy: 0.8731 - val_loss: 0.6275 - val_accuracy: 0.6213\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 0.8735 - val_loss: 0.6053 - val_accuracy: 0.6497\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.8750 - val_loss: 0.6154 - val_accuracy: 0.6407\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.8698 - val_loss: 0.6796 - val_accuracy: 0.6243\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 0.8739 - val_loss: 0.6987 - val_accuracy: 0.6243\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0140 - accuracy: 0.8716 - val_loss: 0.7085 - val_accuracy: 0.6033\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0229 - accuracy: 0.8705 - val_loss: 0.7659 - val_accuracy: 0.6063\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0093 - accuracy: 0.8750 - val_loss: 0.6574 - val_accuracy: 0.6347\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0038 - accuracy: 0.8757 - val_loss: 0.6575 - val_accuracy: 0.6287\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0340 - accuracy: 0.8668 - val_loss: 0.6476 - val_accuracy: 0.6362\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0296 - accuracy: 0.8686 - val_loss: 0.6173 - val_accuracy: 0.6467\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0024 - accuracy: 0.8765 - val_loss: 0.7207 - val_accuracy: 0.6078\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0038 - accuracy: 0.8761 - val_loss: 0.6607 - val_accuracy: 0.6243\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.8739 - val_loss: 0.6477 - val_accuracy: 0.6437\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0049 - accuracy: 0.8761 - val_loss: 0.7481 - val_accuracy: 0.6093\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0087 - accuracy: 0.8739 - val_loss: 0.7303 - val_accuracy: 0.6213\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0049 - accuracy: 0.8761 - val_loss: 0.7308 - val_accuracy: 0.6183\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0397 - accuracy: 0.8641 - val_loss: 0.6891 - val_accuracy: 0.6632\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0455 - accuracy: 0.8604 - val_loss: 0.6436 - val_accuracy: 0.6497\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0198 - accuracy: 0.8690 - val_loss: 0.6608 - val_accuracy: 0.6257\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0160 - accuracy: 0.8713 - val_loss: 0.6503 - val_accuracy: 0.6377\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0023 - accuracy: 0.8761 - val_loss: 0.6671 - val_accuracy: 0.6632\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0025 - accuracy: 0.8765 - val_loss: 0.7613 - val_accuracy: 0.6108\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0025 - accuracy: 0.8761 - val_loss: 0.6795 - val_accuracy: 0.6572\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0062 - accuracy: 0.8746 - val_loss: 0.6459 - val_accuracy: 0.6377\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0087 - accuracy: 0.8757 - val_loss: 0.7433 - val_accuracy: 0.6317\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0023 - accuracy: 0.8761 - val_loss: 0.7426 - val_accuracy: 0.6467\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 0.8757 - val_loss: 0.7581 - val_accuracy: 0.6617\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0027 - accuracy: 0.8757 - val_loss: 0.6963 - val_accuracy: 0.6602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0134 - accuracy: 0.8743 - val_loss: 0.7266 - val_accuracy: 0.6497\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0169 - accuracy: 0.8724 - val_loss: 0.8465 - val_accuracy: 0.6093\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0166 - accuracy: 0.8713 - val_loss: 0.7032 - val_accuracy: 0.6467\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - accuracy: 0.8743 - val_loss: 0.6690 - val_accuracy: 0.6437\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0017 - accuracy: 0.8769 - val_loss: 0.6993 - val_accuracy: 0.6392\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 2.1103e-04 - accuracy: 0.8769 - val_loss: 0.7177 - val_accuracy: 0.6722\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0037 - accuracy: 0.8754 - val_loss: 0.7785 - val_accuracy: 0.6332\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.8739 - val_loss: 0.7174 - val_accuracy: 0.6482\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 0.8754 - val_loss: 0.7749 - val_accuracy: 0.6392\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 0.8754 - val_loss: 0.8135 - val_accuracy: 0.6392\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0336 - accuracy: 0.8653 - val_loss: 0.8074 - val_accuracy: 0.6153\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0028 - accuracy: 0.8765 - val_loss: 0.7355 - val_accuracy: 0.6497\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 7.7835e-04 - accuracy: 0.8772 - val_loss: 0.7306 - val_accuracy: 0.6512\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 1.5458e-04 - accuracy: 0.8769 - val_loss: 0.7139 - val_accuracy: 0.6482\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 8.5123e-04 - accuracy: 0.8769 - val_loss: 0.7182 - val_accuracy: 0.6482\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0012 - accuracy: 0.8765 - val_loss: 0.7282 - val_accuracy: 0.6467\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0067 - accuracy: 0.8746 - val_loss: 0.7068 - val_accuracy: 0.6512\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0063 - accuracy: 0.8746 - val_loss: 0.7574 - val_accuracy: 0.6347\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0035 - accuracy: 0.8757 - val_loss: 0.7563 - val_accuracy: 0.6213\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0136 - accuracy: 0.8728 - val_loss: 0.9466 - val_accuracy: 0.6722\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0356 - accuracy: 0.8660 - val_loss: 0.7879 - val_accuracy: 0.6452\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0059 - accuracy: 0.8750 - val_loss: 0.8954 - val_accuracy: 0.5973\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 0.8757 - val_loss: 0.7113 - val_accuracy: 0.6377\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 3.5243e-04 - accuracy: 0.8769 - val_loss: 0.7588 - val_accuracy: 0.6692\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0010 - accuracy: 0.8765 - val_loss: 0.7599 - val_accuracy: 0.6617\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 3.0459e-04 - accuracy: 0.8769 - val_loss: 0.7742 - val_accuracy: 0.6392\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 4.3175e-04 - accuracy: 0.8765 - val_loss: 0.7597 - val_accuracy: 0.6497\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 1.9378e-04 - accuracy: 0.8769 - val_loss: 0.7402 - val_accuracy: 0.6512\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0087 - accuracy: 0.8746 - val_loss: 0.7793 - val_accuracy: 0.6617\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0062 - accuracy: 0.8750 - val_loss: 0.7564 - val_accuracy: 0.6362\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0140 - accuracy: 0.8728 - val_loss: 0.7983 - val_accuracy: 0.6287\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0025 - accuracy: 0.8761 - val_loss: 0.8564 - val_accuracy: 0.6677\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0080 - accuracy: 0.8746 - val_loss: 0.9151 - val_accuracy: 0.6587\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0140 - accuracy: 0.8716 - val_loss: 0.7606 - val_accuracy: 0.6572\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0044 - accuracy: 0.8757 - val_loss: 0.7597 - val_accuracy: 0.6557\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0091 - accuracy: 0.8735 - val_loss: 0.8450 - val_accuracy: 0.6437\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0036 - accuracy: 0.8761 - val_loss: 0.8336 - val_accuracy: 0.6362\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0032 - accuracy: 0.8761 - val_loss: 0.8200 - val_accuracy: 0.6482\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 0.8626 - val_loss: 1.0566 - val_accuracy: 0.5763\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0452 - accuracy: 0.8615 - val_loss: 0.8019 - val_accuracy: 0.6662\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0073 - accuracy: 0.8746 - val_loss: 0.7532 - val_accuracy: 0.6422\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0015 - accuracy: 0.8765 - val_loss: 0.7744 - val_accuracy: 0.6497\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(trainX, trainY, epochs=100, validation_split=0.2)\n",
    "\n",
    "val_acc = history.history['val_accuracy']\n",
    "best_epoch = val_acc.index(max(val_acc))+1\n",
    "print(best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "id": "680575c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/23\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 1.0221 - accuracy: 0.2788 - val_loss: 0.9721 - val_accuracy: 0.4895\n",
      "Epoch 2/23\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.9454 - accuracy: 0.4618 - val_loss: 0.8216 - val_accuracy: 0.5165\n",
      "Epoch 3/23\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.8010 - accuracy: 0.5284 - val_loss: 0.6805 - val_accuracy: 0.5719\n",
      "Epoch 4/23\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6460 - accuracy: 0.6063 - val_loss: 0.5660 - val_accuracy: 0.6243\n",
      "Epoch 5/23\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.5289 - accuracy: 0.6497 - val_loss: 0.5245 - val_accuracy: 0.6228\n",
      "Epoch 6/23\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4256 - accuracy: 0.6875 - val_loss: 0.4701 - val_accuracy: 0.6467\n",
      "Epoch 7/23\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3839 - accuracy: 0.7013 - val_loss: 0.5096 - val_accuracy: 0.6198\n",
      "Epoch 8/23\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3408 - accuracy: 0.7227 - val_loss: 0.4353 - val_accuracy: 0.6452\n",
      "Epoch 9/23\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3026 - accuracy: 0.7425 - val_loss: 0.4306 - val_accuracy: 0.6437\n",
      "Epoch 10/23\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2645 - accuracy: 0.7567 - val_loss: 0.4696 - val_accuracy: 0.6347\n",
      "Epoch 11/23\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2386 - accuracy: 0.7706 - val_loss: 0.4712 - val_accuracy: 0.6317\n",
      "Epoch 12/23\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2079 - accuracy: 0.7882 - val_loss: 0.5952 - val_accuracy: 0.5973\n",
      "Epoch 13/23\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2015 - accuracy: 0.7923 - val_loss: 0.4624 - val_accuracy: 0.6467\n",
      "Epoch 14/23\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1747 - accuracy: 0.8073 - val_loss: 0.4818 - val_accuracy: 0.6377\n",
      "Epoch 15/23\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1549 - accuracy: 0.8095 - val_loss: 0.4751 - val_accuracy: 0.6437\n",
      "Epoch 16/23\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1205 - accuracy: 0.8271 - val_loss: 0.4787 - val_accuracy: 0.6392\n",
      "Epoch 17/23\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1136 - accuracy: 0.8290 - val_loss: 0.6066 - val_accuracy: 0.6183\n",
      "Epoch 18/23\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1015 - accuracy: 0.8357 - val_loss: 0.6362 - val_accuracy: 0.6093\n",
      "Epoch 19/23\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1033 - accuracy: 0.8346 - val_loss: 0.4996 - val_accuracy: 0.6587\n",
      "Epoch 20/23\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0740 - accuracy: 0.8488 - val_loss: 0.5146 - val_accuracy: 0.6497\n",
      "Epoch 21/23\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0616 - accuracy: 0.8544 - val_loss: 0.5163 - val_accuracy: 0.6527\n",
      "Epoch 22/23\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0581 - accuracy: 0.8559 - val_loss: 0.5188 - val_accuracy: 0.6542\n",
      "Epoch 23/23\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0472 - accuracy: 0.8615 - val_loss: 0.5328 - val_accuracy: 0.6497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12a712d6740>"
      ]
     },
     "execution_count": 1001,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "hypermodel.fit(trainX, trainY, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "id": "20c4aede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 2ms/step - loss: 0.4841 - accuracy: 0.6683\n",
      "[test loss, test accuracy]: [0.48412856459617615, 0.6682634949684143]\n"
     ]
    }
   ],
   "source": [
    "eval_result = hypermodel.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c128c93",
   "metadata": {},
   "source": [
    "### Prepare for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "698281c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "6891b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = big_data[[\"Electronic\", \"Experimental\", \"Folk\", \"Hip-Hop\", \"International\", \"Punk\", \"Rock\"]]\n",
    "X = big_data[[\"mfcc\", \"chroma\", \"spectral\"]]\n",
    "mfcc = np.array(X['mfcc'].values.tolist())\n",
    "steps = mfcc.shape[1]\n",
    "features = mfcc.shape[2]\n",
    "mfcc = mfcc.reshape((-1,steps,features))\n",
    "trainX, testX, trainY, testY = train_test_split(mfcc, Y, stratify=Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8695d78",
   "metadata": {},
   "source": [
    "### Tuned LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "b557b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=(10, 469)))\n",
    "    \n",
    "    hp_units = hp.Int('units', min_value=50, max_value=250, step=50)\n",
    "    hp_drop = hp.Float('drop', min_value=0, max_value=0.5, step=0.1)\n",
    "    model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=hp_units, dropout=hp_drop)))\n",
    "    \n",
    "    model.add(keras.layers.Dense(416, activation='relu'))\n",
    "    hp_units1 = hp.Int('units1', min_value=50, max_value=250, step=50)\n",
    "    model.add(keras.layers.Dense(units=hp_units))\n",
    "    model.add(keras.layers.Dense(7))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.CategoricalHinge(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "9e030928",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3)\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "d3505280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 09s]\n",
      "val_accuracy: 0.667664647102356\n",
      "\n",
      "Best val_accuracy So Far: 0.67514967918396\n",
      "Total elapsed time: 00h 03m 01s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(trainX, trainY, epochs=50, validation_split=0.2, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "773f1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "221d168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "84/84 [==============================] - 3s 14ms/step - loss: 0.7758 - accuracy: 0.5449 - val_loss: 0.5769 - val_accuracy: 0.6228\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.3927 - accuracy: 0.7100 - val_loss: 0.4987 - val_accuracy: 0.6392\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.2966 - accuracy: 0.7534 - val_loss: 0.4723 - val_accuracy: 0.6482\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.2213 - accuracy: 0.7908 - val_loss: 0.4786 - val_accuracy: 0.6452\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.1703 - accuracy: 0.8132 - val_loss: 0.4770 - val_accuracy: 0.6437\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.1223 - accuracy: 0.8376 - val_loss: 0.4795 - val_accuracy: 0.6467\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0734 - accuracy: 0.8540 - val_loss: 0.4920 - val_accuracy: 0.6422\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0549 - accuracy: 0.8641 - val_loss: 0.4976 - val_accuracy: 0.6527\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0365 - accuracy: 0.8668 - val_loss: 0.5072 - val_accuracy: 0.6497\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0275 - accuracy: 0.8694 - val_loss: 0.5064 - val_accuracy: 0.6437\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0146 - accuracy: 0.8724 - val_loss: 0.5178 - val_accuracy: 0.6482\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0143 - accuracy: 0.8757 - val_loss: 0.5163 - val_accuracy: 0.6452\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0095 - accuracy: 0.8772 - val_loss: 0.5208 - val_accuracy: 0.6452\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0088 - accuracy: 0.8757 - val_loss: 0.5268 - val_accuracy: 0.6332\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0053 - accuracy: 0.8735 - val_loss: 0.5240 - val_accuracy: 0.6377\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0043 - accuracy: 0.8765 - val_loss: 0.5245 - val_accuracy: 0.6437\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0043 - accuracy: 0.8757 - val_loss: 0.5421 - val_accuracy: 0.6362\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0053 - accuracy: 0.8735 - val_loss: 0.5352 - val_accuracy: 0.6452\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0043 - accuracy: 0.8743 - val_loss: 0.5399 - val_accuracy: 0.6407\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.8735 - val_loss: 0.5342 - val_accuracy: 0.6512\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0022 - accuracy: 0.8772 - val_loss: 0.5586 - val_accuracy: 0.6482\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0035 - accuracy: 0.8750 - val_loss: 0.5351 - val_accuracy: 0.6542\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0049 - accuracy: 0.8750 - val_loss: 0.5427 - val_accuracy: 0.6527\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0066 - accuracy: 0.8772 - val_loss: 0.5533 - val_accuracy: 0.6497\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0062 - accuracy: 0.8739 - val_loss: 0.5508 - val_accuracy: 0.6437\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0056 - accuracy: 0.8769 - val_loss: 0.5780 - val_accuracy: 0.6332\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0050 - accuracy: 0.8735 - val_loss: 0.5711 - val_accuracy: 0.6527\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0080 - accuracy: 0.8750 - val_loss: 0.5962 - val_accuracy: 0.6452\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0067 - accuracy: 0.8791 - val_loss: 0.5941 - val_accuracy: 0.6572\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0070 - accuracy: 0.8772 - val_loss: 0.5991 - val_accuracy: 0.6467\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0033 - accuracy: 0.8739 - val_loss: 0.5937 - val_accuracy: 0.6482\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 0.8750 - val_loss: 0.5830 - val_accuracy: 0.6542\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0059 - accuracy: 0.8739 - val_loss: 0.6114 - val_accuracy: 0.6392\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0059 - accuracy: 0.8731 - val_loss: 0.6303 - val_accuracy: 0.6347\n",
      "Epoch 35/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0073 - accuracy: 0.8724 - val_loss: 0.6243 - val_accuracy: 0.6422\n",
      "Epoch 36/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0040 - accuracy: 0.8739 - val_loss: 0.6291 - val_accuracy: 0.6407\n",
      "Epoch 37/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0045 - accuracy: 0.8728 - val_loss: 0.6353 - val_accuracy: 0.6497\n",
      "Epoch 38/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0028 - accuracy: 0.8739 - val_loss: 0.6334 - val_accuracy: 0.6452\n",
      "Epoch 39/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.8735 - val_loss: 0.6296 - val_accuracy: 0.6377\n",
      "Epoch 40/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0044 - accuracy: 0.8739 - val_loss: 0.6471 - val_accuracy: 0.6362\n",
      "Epoch 41/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.8735 - val_loss: 0.6298 - val_accuracy: 0.6407\n",
      "Epoch 42/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0035 - accuracy: 0.8731 - val_loss: 0.6203 - val_accuracy: 0.6482\n",
      "Epoch 43/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0019 - accuracy: 0.8735 - val_loss: 0.6319 - val_accuracy: 0.6437\n",
      "Epoch 44/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0044 - accuracy: 0.8735 - val_loss: 0.6403 - val_accuracy: 0.6332\n",
      "Epoch 45/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0034 - accuracy: 0.8739 - val_loss: 0.6602 - val_accuracy: 0.6272\n",
      "Epoch 46/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0062 - accuracy: 0.8728 - val_loss: 0.6477 - val_accuracy: 0.6437\n",
      "Epoch 47/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0055 - accuracy: 0.8731 - val_loss: 0.6664 - val_accuracy: 0.6392\n",
      "Epoch 48/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0090 - accuracy: 0.8731 - val_loss: 0.6850 - val_accuracy: 0.6467\n",
      "Epoch 49/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0104 - accuracy: 0.8713 - val_loss: 0.6874 - val_accuracy: 0.6452\n",
      "Epoch 50/50\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0042 - accuracy: 0.8739 - val_loss: 0.6705 - val_accuracy: 0.6452\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(trainX, trainY, epochs=50, validation_split=0.2)\n",
    "\n",
    "val_acc = history.history['val_accuracy']\n",
    "best_epoch = val_acc.index(max(val_acc))+1\n",
    "print(best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "dcf91284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/29\n",
      "84/84 [==============================] - 3s 16ms/step - loss: 0.8021 - accuracy: 0.5153 - val_loss: 0.5854 - val_accuracy: 0.6272\n",
      "Epoch 2/29\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.3962 - accuracy: 0.7141 - val_loss: 0.5029 - val_accuracy: 0.6452\n",
      "Epoch 3/29\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.2954 - accuracy: 0.7552 - val_loss: 0.4952 - val_accuracy: 0.6392\n",
      "Epoch 4/29\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.2304 - accuracy: 0.7934 - val_loss: 0.4865 - val_accuracy: 0.6422\n",
      "Epoch 5/29\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.1731 - accuracy: 0.8185 - val_loss: 0.4861 - val_accuracy: 0.6632\n",
      "Epoch 6/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.1255 - accuracy: 0.8357 - val_loss: 0.4904 - val_accuracy: 0.6482\n",
      "Epoch 7/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0844 - accuracy: 0.8507 - val_loss: 0.4887 - val_accuracy: 0.6512\n",
      "Epoch 8/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0591 - accuracy: 0.8619 - val_loss: 0.4951 - val_accuracy: 0.6482\n",
      "Epoch 9/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0363 - accuracy: 0.8660 - val_loss: 0.4944 - val_accuracy: 0.6617\n",
      "Epoch 10/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0236 - accuracy: 0.8724 - val_loss: 0.5135 - val_accuracy: 0.6647\n",
      "Epoch 11/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0164 - accuracy: 0.8765 - val_loss: 0.5217 - val_accuracy: 0.6452\n",
      "Epoch 12/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0096 - accuracy: 0.8750 - val_loss: 0.5220 - val_accuracy: 0.6527\n",
      "Epoch 13/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0071 - accuracy: 0.8739 - val_loss: 0.5275 - val_accuracy: 0.6557\n",
      "Epoch 14/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0074 - accuracy: 0.8806 - val_loss: 0.5361 - val_accuracy: 0.6512\n",
      "Epoch 15/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0052 - accuracy: 0.8772 - val_loss: 0.5301 - val_accuracy: 0.6452\n",
      "Epoch 16/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0051 - accuracy: 0.8784 - val_loss: 0.5259 - val_accuracy: 0.6482\n",
      "Epoch 17/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0032 - accuracy: 0.8799 - val_loss: 0.5375 - val_accuracy: 0.6587\n",
      "Epoch 18/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0039 - accuracy: 0.8772 - val_loss: 0.5353 - val_accuracy: 0.6467\n",
      "Epoch 19/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0017 - accuracy: 0.8757 - val_loss: 0.5327 - val_accuracy: 0.6647\n",
      "Epoch 20/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0040 - accuracy: 0.8814 - val_loss: 0.5436 - val_accuracy: 0.6572\n",
      "Epoch 21/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0049 - accuracy: 0.8754 - val_loss: 0.5402 - val_accuracy: 0.6437\n",
      "Epoch 22/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.8806 - val_loss: 0.5338 - val_accuracy: 0.6662\n",
      "Epoch 23/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.8817 - val_loss: 0.5618 - val_accuracy: 0.6557\n",
      "Epoch 24/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0019 - accuracy: 0.8787 - val_loss: 0.5647 - val_accuracy: 0.6557\n",
      "Epoch 25/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0028 - accuracy: 0.8750 - val_loss: 0.5548 - val_accuracy: 0.6512\n",
      "Epoch 26/29\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.8761 - val_loss: 0.5719 - val_accuracy: 0.6542\n",
      "Epoch 27/29\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 0.8791 - val_loss: 0.5673 - val_accuracy: 0.6392\n",
      "Epoch 28/29\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 0.8743 - val_loss: 0.5875 - val_accuracy: 0.6243\n",
      "Epoch 29/29\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0069 - accuracy: 0.8754 - val_loss: 0.5732 - val_accuracy: 0.6422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x129384cf640>"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "hypermodel.fit(trainX, trainY, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "ba490c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 10ms/step - loss: 0.5300 - accuracy: 0.6695\n",
      "[test loss, test accuracy]: [0.5299615263938904, 0.6694610714912415]\n"
     ]
    }
   ],
   "source": [
    "eval_result = hypermodel.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50afd960",
   "metadata": {},
   "source": [
    "### Baseline LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "1056d6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "53/53 [==============================] - 2s 16ms/step - loss: 1.0069 - accuracy: 0.4425 - val_loss: 0.5838 - val_accuracy: 0.6240\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.6000 - accuracy: 0.6344 - val_loss: 0.5196 - val_accuracy: 0.6539\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.5000 - accuracy: 0.6680 - val_loss: 0.5146 - val_accuracy: 0.6407\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4341 - accuracy: 0.6901 - val_loss: 0.4880 - val_accuracy: 0.6551\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3935 - accuracy: 0.7057 - val_loss: 0.4796 - val_accuracy: 0.6695\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3604 - accuracy: 0.7266 - val_loss: 0.4772 - val_accuracy: 0.6671\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3127 - accuracy: 0.7443 - val_loss: 0.4823 - val_accuracy: 0.6563\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2874 - accuracy: 0.7566 - val_loss: 0.4660 - val_accuracy: 0.6623\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2643 - accuracy: 0.7704 - val_loss: 0.4655 - val_accuracy: 0.6659\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2551 - accuracy: 0.7763 - val_loss: 0.4759 - val_accuracy: 0.6647\n",
      "27/27 [==============================] - 0s 2ms/step - loss: 0.4759 - accuracy: 0.6647\n",
      "[test loss, test accuracy]: [0.4758921265602112, 0.6646706461906433]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.Input(shape=(10, 469)),\n",
    "  tf.keras.layers.LSTM(100, dropout=0.2),\n",
    "  tf.keras.layers.Dense(7)\n",
    "])\n",
    "model.compile(loss=\"CategoricalHinge\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=10, batch_size=64)\n",
    "eval_result = model.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9f343686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "53/53 [==============================] - 3s 21ms/step - loss: 0.9546 - accuracy: 0.4674 - val_loss: 0.5447 - val_accuracy: 0.6371\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.5534 - accuracy: 0.6569 - val_loss: 0.4827 - val_accuracy: 0.6527\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4475 - accuracy: 0.6883 - val_loss: 0.4702 - val_accuracy: 0.6635\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3993 - accuracy: 0.7018 - val_loss: 0.4634 - val_accuracy: 0.6754\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3611 - accuracy: 0.7228 - val_loss: 0.4664 - val_accuracy: 0.6599\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3103 - accuracy: 0.7509 - val_loss: 0.4670 - val_accuracy: 0.6611\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2915 - accuracy: 0.7509 - val_loss: 0.4600 - val_accuracy: 0.6707\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2664 - accuracy: 0.7620 - val_loss: 0.4467 - val_accuracy: 0.6778\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2539 - accuracy: 0.7701 - val_loss: 0.4477 - val_accuracy: 0.6802\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2361 - accuracy: 0.7817 - val_loss: 0.4371 - val_accuracy: 0.6826\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 0.4371 - accuracy: 0.6826\n",
      "[test loss, test accuracy]: [0.4371247887611389, 0.682634711265564]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.Input(shape=(10, 469)),\n",
    "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, dropout=0.2)),\n",
    "  tf.keras.layers.Dense(7)\n",
    "])\n",
    "model.compile(loss=\"CategoricalHinge\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=10, batch_size=64)\n",
    "eval_result = model.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6e70b",
   "metadata": {},
   "source": [
    "### Tuned Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "2423ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "1397a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(7, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "aaa84b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 10, 469)]    0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_32 (LayerN  (None, 10, 469)     938         ['input_9[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_16 (Multi  (None, 10, 469)     1924565     ['layer_normalization_32[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 10, 469)      0           ['multi_head_attention_16[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_32 (TFOpL  (None, 10, 469)     0           ['dropout_36[0][0]',             \n",
      " ambda)                                                           'input_9[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_33 (LayerN  (None, 10, 469)     938         ['tf.__operators__.add_32[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_32 (Conv1D)             (None, 10, 4)        1880        ['layer_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 10, 4)        0           ['conv1d_32[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_33 (Conv1D)             (None, 10, 469)      2345        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_33 (TFOpL  (None, 10, 469)     0           ['conv1d_33[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_32[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_34 (LayerN  (None, 10, 469)     938         ['tf.__operators__.add_33[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_17 (Multi  (None, 10, 469)     1924565     ['layer_normalization_34[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 10, 469)      0           ['multi_head_attention_17[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_34 (TFOpL  (None, 10, 469)     0           ['dropout_38[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_33[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_35 (LayerN  (None, 10, 469)     938         ['tf.__operators__.add_34[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_34 (Conv1D)             (None, 10, 4)        1880        ['layer_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_39 (Dropout)           (None, 10, 4)        0           ['conv1d_34[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_35 (Conv1D)             (None, 10, 469)      2345        ['dropout_39[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_35 (TFOpL  (None, 10, 469)     0           ['conv1d_35[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_34[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_36 (LayerN  (None, 10, 469)     938         ['tf.__operators__.add_35[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_18 (Multi  (None, 10, 469)     1924565     ['layer_normalization_36[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_40 (Dropout)           (None, 10, 469)      0           ['multi_head_attention_18[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_36 (TFOpL  (None, 10, 469)     0           ['dropout_40[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_35[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_37 (LayerN  (None, 10, 469)     938         ['tf.__operators__.add_36[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_36 (Conv1D)             (None, 10, 4)        1880        ['layer_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)           (None, 10, 4)        0           ['conv1d_36[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_37 (Conv1D)             (None, 10, 469)      2345        ['dropout_41[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_37 (TFOpL  (None, 10, 469)     0           ['conv1d_37[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_36[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_38 (LayerN  (None, 10, 469)     938         ['tf.__operators__.add_37[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_19 (Multi  (None, 10, 469)     1924565     ['layer_normalization_38[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_42 (Dropout)           (None, 10, 469)      0           ['multi_head_attention_19[0][0]']\n",
      "                                                                                                  \n",
      " tf.__operators__.add_38 (TFOpL  (None, 10, 469)     0           ['dropout_42[0][0]',             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ambda)                                                           'tf.__operators__.add_37[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_39 (LayerN  (None, 10, 469)     938         ['tf.__operators__.add_38[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_38 (Conv1D)             (None, 10, 4)        1880        ['layer_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_43 (Dropout)           (None, 10, 4)        0           ['conv1d_38[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_39 (Conv1D)             (None, 10, 469)      2345        ['dropout_43[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_39 (TFOpL  (None, 10, 469)     0           ['conv1d_39[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_38[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_4 (Gl  (None, 10)          0           ['tf.__operators__.add_39[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 128)          1408        ['global_average_pooling1d_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_44 (Dropout)           (None, 128)          0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 7)            903         ['dropout_44[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,724,975\n",
      "Trainable params: 7,724,975\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "42/42 [==============================] - 5s 36ms/step - loss: 1.7832 - accuracy: 0.0939 - val_loss: 1.7249 - val_accuracy: 0.1287\n",
      "Epoch 2/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.7613 - accuracy: 0.1078 - val_loss: 1.7241 - val_accuracy: 0.1302\n",
      "Epoch 3/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.7593 - accuracy: 0.1044 - val_loss: 1.7185 - val_accuracy: 0.1362\n",
      "Epoch 4/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.7373 - accuracy: 0.1186 - val_loss: 1.7101 - val_accuracy: 0.1362\n",
      "Epoch 5/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.7183 - accuracy: 0.1295 - val_loss: 1.6675 - val_accuracy: 0.1617\n",
      "Epoch 6/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.6713 - accuracy: 0.1512 - val_loss: 1.5843 - val_accuracy: 0.2036\n",
      "Epoch 7/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.6000 - accuracy: 0.1901 - val_loss: 1.5531 - val_accuracy: 0.2156\n",
      "Epoch 8/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.5767 - accuracy: 0.2010 - val_loss: 1.5342 - val_accuracy: 0.2126\n",
      "Epoch 9/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.5616 - accuracy: 0.2058 - val_loss: 1.4802 - val_accuracy: 0.2335\n",
      "Epoch 10/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.5253 - accuracy: 0.2242 - val_loss: 1.4624 - val_accuracy: 0.2485\n",
      "Epoch 11/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.5178 - accuracy: 0.2234 - val_loss: 1.3882 - val_accuracy: 0.2784\n",
      "Epoch 12/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.4598 - accuracy: 0.2507 - val_loss: 1.3375 - val_accuracy: 0.3069\n",
      "Epoch 13/200\n",
      "42/42 [==============================] - 1s 22ms/step - loss: 1.4018 - accuracy: 0.2762 - val_loss: 1.3496 - val_accuracy: 0.2889\n",
      "Epoch 14/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.3729 - accuracy: 0.2826 - val_loss: 1.3361 - val_accuracy: 0.2919\n",
      "Epoch 15/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.3430 - accuracy: 0.2912 - val_loss: 1.2776 - val_accuracy: 0.3099\n",
      "Epoch 16/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.3233 - accuracy: 0.2949 - val_loss: 1.2933 - val_accuracy: 0.3099\n",
      "Epoch 17/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.2797 - accuracy: 0.3170 - val_loss: 1.2491 - val_accuracy: 0.3219\n",
      "Epoch 18/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.2779 - accuracy: 0.3151 - val_loss: 1.2445 - val_accuracy: 0.3249\n",
      "Epoch 19/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.2411 - accuracy: 0.3286 - val_loss: 1.2018 - val_accuracy: 0.3443\n",
      "Epoch 20/200\n",
      "42/42 [==============================] - 1s 25ms/step - loss: 1.2341 - accuracy: 0.3282 - val_loss: 1.1998 - val_accuracy: 0.3383\n",
      "Epoch 21/200\n",
      "42/42 [==============================] - 1s 24ms/step - loss: 1.2288 - accuracy: 0.3282 - val_loss: 1.1581 - val_accuracy: 0.3518\n",
      "Epoch 22/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.2238 - accuracy: 0.3293 - val_loss: 1.1378 - val_accuracy: 0.3533\n",
      "Epoch 23/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.2164 - accuracy: 0.3290 - val_loss: 1.1185 - val_accuracy: 0.3653\n",
      "Epoch 24/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.2111 - accuracy: 0.3312 - val_loss: 1.1108 - val_accuracy: 0.3653\n",
      "Epoch 25/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.1795 - accuracy: 0.3335 - val_loss: 1.1274 - val_accuracy: 0.3488\n",
      "Epoch 26/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.1705 - accuracy: 0.3297 - val_loss: 1.1062 - val_accuracy: 0.3503\n",
      "Epoch 27/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.1797 - accuracy: 0.3267 - val_loss: 1.1056 - val_accuracy: 0.3473\n",
      "Epoch 28/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.1790 - accuracy: 0.3260 - val_loss: 1.0816 - val_accuracy: 0.3563\n",
      "Epoch 29/200\n",
      "42/42 [==============================] - 1s 25ms/step - loss: 1.1551 - accuracy: 0.3290 - val_loss: 1.0455 - val_accuracy: 0.3638\n",
      "Epoch 30/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.1285 - accuracy: 0.3398 - val_loss: 1.0558 - val_accuracy: 0.3578\n",
      "Epoch 31/200\n",
      "42/42 [==============================] - 1s 24ms/step - loss: 1.1386 - accuracy: 0.3350 - val_loss: 1.0613 - val_accuracy: 0.3578\n",
      "Epoch 32/200\n",
      "42/42 [==============================] - 1s 22ms/step - loss: 1.1426 - accuracy: 0.3323 - val_loss: 1.0735 - val_accuracy: 0.3563\n",
      "Epoch 33/200\n",
      "42/42 [==============================] - 1s 22ms/step - loss: 1.1533 - accuracy: 0.3267 - val_loss: 1.0775 - val_accuracy: 0.3488\n",
      "Epoch 34/200\n",
      "42/42 [==============================] - 1s 22ms/step - loss: 1.1642 - accuracy: 0.3200 - val_loss: 1.0599 - val_accuracy: 0.3578\n",
      "Epoch 35/200\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.1565 - accuracy: 0.3204 - val_loss: 1.0569 - val_accuracy: 0.3533\n",
      "Epoch 36/200\n",
      "42/42 [==============================] - 1s 22ms/step - loss: 1.1416 - accuracy: 0.3200 - val_loss: 1.0569 - val_accuracy: 0.3533\n",
      "Epoch 37/200\n",
      "42/42 [==============================] - 1s 22ms/step - loss: 1.1312 - accuracy: 0.3234 - val_loss: 1.0499 - val_accuracy: 0.3548\n",
      "Epoch 38/200\n",
      "42/42 [==============================] - 1s 22ms/step - loss: 1.1203 - accuracy: 0.3278 - val_loss: 1.0608 - val_accuracy: 0.3488\n",
      "Epoch 39/200\n",
      "42/42 [==============================] - 1s 24ms/step - loss: 1.1363 - accuracy: 0.3215 - val_loss: 1.0526 - val_accuracy: 0.3533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 6ms/step - loss: 1.1008 - accuracy: 0.3437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1007895469665527, 0.34371256828308105]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = trainX.shape[1:]\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "41316f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_builder(hp):\n",
    "    \n",
    "    hp_blocks = hp.Int(\"blocks\", min_value=4, max_value=5, step=2)\n",
    "    hp_units = hp.Int('units', min_value=64, max_value=256, step=64)\n",
    "    hp_drop = hp.Float('drop', min_value=0.1, max_value=0.3, step=0.1)\n",
    "    hp_drop1 = hp.Float('drop1', min_value=0.1, max_value=0.3, step=0.1)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "                      \n",
    "    model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=hp_blocks,\n",
    "    mlp_units=[hp_units],\n",
    "    mlp_dropout=hp_drop,\n",
    "    dropout=hp_drop1,\n",
    ")\n",
    "    model.compile(\n",
    "    loss=keras.losses.CategoricalHinge(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "b76d2a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=20,\n",
    "                     factor=3)\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "645c5c97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 40s]\n",
      "val_accuracy: 0.26047903299331665\n",
      "\n",
      "Best val_accuracy So Far: 0.5883233547210693\n",
      "Total elapsed time: 00h 08m 32s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(trainX, trainY, epochs=20, validation_split=0.2, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "4e0557dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "3573885c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "84/84 [==============================] - 5s 26ms/step - loss: 1.5476 - accuracy: 0.1557 - val_loss: 1.5222 - val_accuracy: 0.1632\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 1.4415 - accuracy: 0.1976 - val_loss: 1.2599 - val_accuracy: 0.2620\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 1.2937 - accuracy: 0.2788 - val_loss: 1.1580 - val_accuracy: 0.3278\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 1.2073 - accuracy: 0.3185 - val_loss: 1.0941 - val_accuracy: 0.3623\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 1.0491 - accuracy: 0.3896 - val_loss: 0.9245 - val_accuracy: 0.4311\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.9159 - accuracy: 0.4386 - val_loss: 0.8286 - val_accuracy: 0.4716\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.8627 - accuracy: 0.4581 - val_loss: 0.8126 - val_accuracy: 0.4731\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.7797 - accuracy: 0.4929 - val_loss: 0.7395 - val_accuracy: 0.5015\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.7359 - accuracy: 0.5105 - val_loss: 0.7226 - val_accuracy: 0.5135\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.7038 - accuracy: 0.5236 - val_loss: 0.7317 - val_accuracy: 0.5120\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.7074 - accuracy: 0.5236 - val_loss: 0.7489 - val_accuracy: 0.5045\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.7031 - accuracy: 0.5240 - val_loss: 0.7117 - val_accuracy: 0.5240\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6887 - accuracy: 0.5333 - val_loss: 0.7088 - val_accuracy: 0.5240\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6750 - accuracy: 0.5382 - val_loss: 0.7049 - val_accuracy: 0.5254\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6773 - accuracy: 0.5371 - val_loss: 0.7150 - val_accuracy: 0.5210\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6756 - accuracy: 0.5382 - val_loss: 0.7072 - val_accuracy: 0.5240\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6641 - accuracy: 0.5423 - val_loss: 0.7097 - val_accuracy: 0.5240\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - 2s 22ms/step - loss: 0.6671 - accuracy: 0.5423 - val_loss: 0.7019 - val_accuracy: 0.5225\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - 2s 23ms/step - loss: 0.6503 - accuracy: 0.5513 - val_loss: 0.7088 - val_accuracy: 0.5284\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - 2s 21ms/step - loss: 0.6689 - accuracy: 0.5412 - val_loss: 0.6950 - val_accuracy: 0.5329\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6426 - accuracy: 0.5539 - val_loss: 0.6885 - val_accuracy: 0.5329\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6418 - accuracy: 0.5550 - val_loss: 0.7082 - val_accuracy: 0.5254\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6345 - accuracy: 0.5584 - val_loss: 0.6985 - val_accuracy: 0.5254\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6333 - accuracy: 0.5591 - val_loss: 0.6968 - val_accuracy: 0.5284\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6224 - accuracy: 0.5636 - val_loss: 0.7088 - val_accuracy: 0.5254\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6253 - accuracy: 0.5625 - val_loss: 0.6820 - val_accuracy: 0.5374\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6037 - accuracy: 0.5730 - val_loss: 0.7056 - val_accuracy: 0.5240\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6144 - accuracy: 0.5666 - val_loss: 0.7042 - val_accuracy: 0.5225\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6174 - accuracy: 0.5674 - val_loss: 0.6926 - val_accuracy: 0.5314\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6034 - accuracy: 0.5726 - val_loss: 0.7011 - val_accuracy: 0.5299\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6124 - accuracy: 0.5696 - val_loss: 0.7010 - val_accuracy: 0.5299\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.5978 - accuracy: 0.5756 - val_loss: 0.6925 - val_accuracy: 0.5359\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6102 - accuracy: 0.5685 - val_loss: 0.7111 - val_accuracy: 0.5225\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6043 - accuracy: 0.5737 - val_loss: 0.7079 - val_accuracy: 0.5269\n",
      "Epoch 35/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.5783 - accuracy: 0.5850 - val_loss: 0.6946 - val_accuracy: 0.5314\n",
      "Epoch 36/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.5823 - accuracy: 0.5820 - val_loss: 0.7033 - val_accuracy: 0.5299\n",
      "Epoch 37/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.5753 - accuracy: 0.5850 - val_loss: 0.6897 - val_accuracy: 0.5329\n",
      "Epoch 38/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.5865 - accuracy: 0.5805 - val_loss: 0.7021 - val_accuracy: 0.5284\n",
      "Epoch 39/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.5900 - accuracy: 0.5797 - val_loss: 0.7146 - val_accuracy: 0.5210\n",
      "Epoch 40/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.5758 - accuracy: 0.5868 - val_loss: 0.6990 - val_accuracy: 0.5299\n",
      "Epoch 41/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.5682 - accuracy: 0.5894 - val_loss: 0.7210 - val_accuracy: 0.5180\n",
      "Epoch 42/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.5748 - accuracy: 0.5876 - val_loss: 0.7093 - val_accuracy: 0.5225\n",
      "Epoch 43/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.5721 - accuracy: 0.5891 - val_loss: 0.7018 - val_accuracy: 0.5269\n",
      "Epoch 44/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.5653 - accuracy: 0.5913 - val_loss: 0.7054 - val_accuracy: 0.5284\n",
      "Epoch 45/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.5640 - accuracy: 0.5913 - val_loss: 0.7061 - val_accuracy: 0.5284\n",
      "Epoch 46/50\n",
      "84/84 [==============================] - 2s 19ms/step - loss: 0.5614 - accuracy: 0.5936 - val_loss: 0.7121 - val_accuracy: 0.5240\n",
      "Epoch 47/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.5653 - accuracy: 0.5909 - val_loss: 0.7125 - val_accuracy: 0.5269\n",
      "Epoch 48/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.5587 - accuracy: 0.5936 - val_loss: 0.7004 - val_accuracy: 0.5284\n",
      "Epoch 49/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.5542 - accuracy: 0.5969 - val_loss: 0.7018 - val_accuracy: 0.5284\n",
      "Epoch 50/50\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.5499 - accuracy: 0.5996 - val_loss: 0.6971 - val_accuracy: 0.5314\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(trainX, trainY, epochs=50, validation_split=0.2)\n",
    "\n",
    "val_acc = history.history['val_accuracy']\n",
    "best_epoch = val_acc.index(max(val_acc))+1\n",
    "print(best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "25ccaef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/26\n",
      "84/84 [==============================] - 5s 27ms/step - loss: 1.6536 - accuracy: 0.1340 - val_loss: 1.6092 - val_accuracy: 0.1392\n",
      "Epoch 2/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 1.5766 - accuracy: 0.1707 - val_loss: 1.4654 - val_accuracy: 0.2260\n",
      "Epoch 3/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 1.4707 - accuracy: 0.2350 - val_loss: 1.3905 - val_accuracy: 0.2650\n",
      "Epoch 4/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 1.3080 - accuracy: 0.3132 - val_loss: 1.1638 - val_accuracy: 0.3578\n",
      "Epoch 5/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 1.1237 - accuracy: 0.3855 - val_loss: 1.0434 - val_accuracy: 0.4222\n",
      "Epoch 6/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 1.0438 - accuracy: 0.4154 - val_loss: 1.0143 - val_accuracy: 0.4237\n",
      "Epoch 7/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.9945 - accuracy: 0.4274 - val_loss: 0.9231 - val_accuracy: 0.4461\n",
      "Epoch 8/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.9105 - accuracy: 0.4540 - val_loss: 0.8816 - val_accuracy: 0.4581\n",
      "Epoch 9/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.8620 - accuracy: 0.4749 - val_loss: 0.8714 - val_accuracy: 0.4641\n",
      "Epoch 10/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.8496 - accuracy: 0.4787 - val_loss: 0.8515 - val_accuracy: 0.4850\n",
      "Epoch 11/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.8332 - accuracy: 0.4813 - val_loss: 0.8206 - val_accuracy: 0.4865\n",
      "Epoch 12/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.8037 - accuracy: 0.4978 - val_loss: 0.8454 - val_accuracy: 0.4820\n",
      "Epoch 13/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.8124 - accuracy: 0.4925 - val_loss: 0.8468 - val_accuracy: 0.4850\n",
      "Epoch 14/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.7964 - accuracy: 0.5034 - val_loss: 0.8640 - val_accuracy: 0.4775\n",
      "Epoch 15/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.7743 - accuracy: 0.5124 - val_loss: 0.8270 - val_accuracy: 0.4880\n",
      "Epoch 16/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.7754 - accuracy: 0.5116 - val_loss: 0.8236 - val_accuracy: 0.4925\n",
      "Epoch 17/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.7696 - accuracy: 0.5097 - val_loss: 0.8279 - val_accuracy: 0.4910\n",
      "Epoch 18/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.7644 - accuracy: 0.5112 - val_loss: 0.8628 - val_accuracy: 0.4805\n",
      "Epoch 19/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.7687 - accuracy: 0.5172 - val_loss: 0.8244 - val_accuracy: 0.4940\n",
      "Epoch 20/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.7423 - accuracy: 0.5262 - val_loss: 0.8215 - val_accuracy: 0.4925\n",
      "Epoch 21/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.7414 - accuracy: 0.5273 - val_loss: 0.8053 - val_accuracy: 0.4985\n",
      "Epoch 22/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.7253 - accuracy: 0.5318 - val_loss: 0.8106 - val_accuracy: 0.4940\n",
      "Epoch 23/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.7235 - accuracy: 0.5284 - val_loss: 0.8261 - val_accuracy: 0.4970\n",
      "Epoch 24/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.7105 - accuracy: 0.5348 - val_loss: 0.7764 - val_accuracy: 0.5000\n",
      "Epoch 25/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6936 - accuracy: 0.5378 - val_loss: 0.7602 - val_accuracy: 0.5000\n",
      "Epoch 26/26\n",
      "84/84 [==============================] - 2s 20ms/step - loss: 0.6826 - accuracy: 0.5487 - val_loss: 0.7746 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x129f60e0400>"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "hypermodel.fit(trainX, trainY, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "43e08359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 6ms/step - loss: 0.7496 - accuracy: 0.5090\n",
      "[test loss, test accuracy]: [0.7496476173400879, 0.5089820623397827]\n"
     ]
    }
   ],
   "source": [
    "eval_result = hypermodel.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3cb87a",
   "metadata": {},
   "source": [
    "### Tuned Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "cf32d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = big_data[[\"Electronic\", \"Experimental\", \"Folk\", \"Hip-Hop\", \"International\", \"Punk\", \"Rock\"]]\n",
    "X = big_data[[\"mfcc\", \"chroma\", \"spectral\"]]\n",
    "mfcc = np.array(X['mfcc'].values.tolist())\n",
    "samples = mfcc.shape[0]\n",
    "steps = mfcc.shape[2]\n",
    "coef = 10\n",
    "mfcc = mfcc[:,:steps*coef].reshape(samples,steps,coef)\n",
    "trainX, testX, trainY, testY = train_test_split(mfcc, Y, stratify=Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "2b4f1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(469, 10)))\n",
    "    \n",
    "    hp_units = hp.Int('units', min_value=32, max_value=128, step=32)\n",
    "    hp_drop = hp.Float('drop', min_value=0, max_value=0.5, step=0.1)\n",
    "    \n",
    "    model.add(keras.layers.SimpleRNN(units=hp_units, dropout=hp_drop))\n",
    "\n",
    "    model.add(keras.layers.Dense(7))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.CategoricalHinge(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "ecd42694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 04m 01s]\n",
      "val_accuracy: 0.30239519476890564\n",
      "\n",
      "Best val_accuracy So Far: 0.636227548122406\n",
      "Total elapsed time: 00h 51m 35s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3)\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(trainX, trainY, epochs=20, validation_split=0.2, callbacks=[stop_early])\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "15322a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "84/84 [==============================] - 25s 290ms/step - loss: 1.2922 - accuracy: 0.3986 - val_loss: 0.8257 - val_accuracy: 0.5808\n",
      "Epoch 2/20\n",
      "84/84 [==============================] - 24s 287ms/step - loss: 0.6702 - accuracy: 0.6213 - val_loss: 0.6461 - val_accuracy: 0.6093\n",
      "Epoch 3/20\n",
      "84/84 [==============================] - 24s 288ms/step - loss: 0.5242 - accuracy: 0.6662 - val_loss: 0.6068 - val_accuracy: 0.6033\n",
      "Epoch 4/20\n",
      "84/84 [==============================] - 24s 288ms/step - loss: 0.4624 - accuracy: 0.6864 - val_loss: 0.6012 - val_accuracy: 0.6048\n",
      "Epoch 5/20\n",
      "84/84 [==============================] - 24s 287ms/step - loss: 0.3978 - accuracy: 0.7227 - val_loss: 0.5551 - val_accuracy: 0.6257\n",
      "Epoch 6/20\n",
      "84/84 [==============================] - 24s 284ms/step - loss: 0.3524 - accuracy: 0.7373 - val_loss: 0.5639 - val_accuracy: 0.6287\n",
      "Epoch 7/20\n",
      "84/84 [==============================] - 24s 283ms/step - loss: 0.3250 - accuracy: 0.7541 - val_loss: 0.5444 - val_accuracy: 0.6497\n",
      "Epoch 8/20\n",
      "84/84 [==============================] - 24s 287ms/step - loss: 0.2970 - accuracy: 0.7605 - val_loss: 0.5541 - val_accuracy: 0.6407\n",
      "Epoch 9/20\n",
      "84/84 [==============================] - 24s 286ms/step - loss: 0.2747 - accuracy: 0.7766 - val_loss: 0.5467 - val_accuracy: 0.6407\n",
      "Epoch 10/20\n",
      "84/84 [==============================] - 24s 284ms/step - loss: 0.2470 - accuracy: 0.7867 - val_loss: 0.5547 - val_accuracy: 0.6527\n",
      "Epoch 11/20\n",
      "84/84 [==============================] - 24s 284ms/step - loss: 0.2224 - accuracy: 0.7964 - val_loss: 0.5584 - val_accuracy: 0.6542\n",
      "Epoch 12/20\n",
      "84/84 [==============================] - 24s 282ms/step - loss: 0.2058 - accuracy: 0.8084 - val_loss: 0.5510 - val_accuracy: 0.6512\n",
      "Epoch 13/20\n",
      "84/84 [==============================] - 24s 284ms/step - loss: 0.1885 - accuracy: 0.8162 - val_loss: 0.5480 - val_accuracy: 0.6632\n",
      "Epoch 14/20\n",
      "84/84 [==============================] - 24s 286ms/step - loss: 0.1733 - accuracy: 0.8263 - val_loss: 0.5518 - val_accuracy: 0.6527\n",
      "Epoch 15/20\n",
      "84/84 [==============================] - 24s 286ms/step - loss: 0.1620 - accuracy: 0.8297 - val_loss: 0.5662 - val_accuracy: 0.6557\n",
      "Epoch 16/20\n",
      "84/84 [==============================] - 24s 288ms/step - loss: 0.1528 - accuracy: 0.8327 - val_loss: 0.5620 - val_accuracy: 0.6662\n",
      "Epoch 17/20\n",
      "84/84 [==============================] - 24s 285ms/step - loss: 0.1294 - accuracy: 0.8394 - val_loss: 0.5685 - val_accuracy: 0.6527\n",
      "Epoch 18/20\n",
      "84/84 [==============================] - 24s 284ms/step - loss: 0.1210 - accuracy: 0.8443 - val_loss: 0.5625 - val_accuracy: 0.6632\n",
      "Epoch 19/20\n",
      "84/84 [==============================] - 24s 285ms/step - loss: 0.1094 - accuracy: 0.8499 - val_loss: 0.5573 - val_accuracy: 0.6647\n",
      "Epoch 20/20\n",
      "84/84 [==============================] - 24s 286ms/step - loss: 0.1056 - accuracy: 0.8544 - val_loss: 0.5649 - val_accuracy: 0.6692\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(trainX, trainY, epochs=20, validation_split=0.2)\n",
    "\n",
    "val_acc = history.history['val_accuracy']\n",
    "best_epoch = val_acc.index(max(val_acc))+1\n",
    "print(best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "5e883a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "84/84 [==============================] - 25s 290ms/step - loss: 1.2571 - accuracy: 0.4210 - val_loss: 0.8462 - val_accuracy: 0.5659\n",
      "Epoch 2/20\n",
      "84/84 [==============================] - 24s 287ms/step - loss: 0.6824 - accuracy: 0.6299 - val_loss: 0.6772 - val_accuracy: 0.6213\n",
      "Epoch 3/20\n",
      "84/84 [==============================] - 24s 288ms/step - loss: 0.5395 - accuracy: 0.6755 - val_loss: 0.6329 - val_accuracy: 0.6078\n",
      "Epoch 4/20\n",
      "84/84 [==============================] - 24s 284ms/step - loss: 0.4691 - accuracy: 0.6995 - val_loss: 0.6099 - val_accuracy: 0.6228\n",
      "Epoch 5/20\n",
      "84/84 [==============================] - 24s 283ms/step - loss: 0.4020 - accuracy: 0.7242 - val_loss: 0.5740 - val_accuracy: 0.6422\n",
      "Epoch 6/20\n",
      "84/84 [==============================] - 24s 284ms/step - loss: 0.3592 - accuracy: 0.7429 - val_loss: 0.5690 - val_accuracy: 0.6422\n",
      "Epoch 7/20\n",
      "84/84 [==============================] - 24s 287ms/step - loss: 0.3299 - accuracy: 0.7534 - val_loss: 0.5574 - val_accuracy: 0.6422\n",
      "Epoch 8/20\n",
      "84/84 [==============================] - 24s 289ms/step - loss: 0.2934 - accuracy: 0.7702 - val_loss: 0.5646 - val_accuracy: 0.6317\n",
      "Epoch 9/20\n",
      "84/84 [==============================] - 24s 285ms/step - loss: 0.2686 - accuracy: 0.7762 - val_loss: 0.5779 - val_accuracy: 0.6302\n",
      "Epoch 10/20\n",
      "84/84 [==============================] - 24s 286ms/step - loss: 0.2455 - accuracy: 0.7867 - val_loss: 0.5658 - val_accuracy: 0.6213\n",
      "Epoch 11/20\n",
      "84/84 [==============================] - 24s 287ms/step - loss: 0.2227 - accuracy: 0.7953 - val_loss: 0.5448 - val_accuracy: 0.6347\n",
      "Epoch 12/20\n",
      "84/84 [==============================] - 24s 285ms/step - loss: 0.2040 - accuracy: 0.8031 - val_loss: 0.5678 - val_accuracy: 0.6272\n",
      "Epoch 13/20\n",
      "84/84 [==============================] - 24s 286ms/step - loss: 0.1858 - accuracy: 0.8103 - val_loss: 0.5612 - val_accuracy: 0.6317\n",
      "Epoch 14/20\n",
      "84/84 [==============================] - 24s 284ms/step - loss: 0.1697 - accuracy: 0.8185 - val_loss: 0.5615 - val_accuracy: 0.6302\n",
      "Epoch 15/20\n",
      "84/84 [==============================] - 24s 290ms/step - loss: 0.1583 - accuracy: 0.8237 - val_loss: 0.5858 - val_accuracy: 0.6302\n",
      "Epoch 16/20\n",
      "84/84 [==============================] - 24s 285ms/step - loss: 0.1421 - accuracy: 0.8346 - val_loss: 0.5965 - val_accuracy: 0.6302\n",
      "Epoch 17/20\n",
      "84/84 [==============================] - 24s 285ms/step - loss: 0.1318 - accuracy: 0.8338 - val_loss: 0.5855 - val_accuracy: 0.6377\n",
      "Epoch 18/20\n",
      "84/84 [==============================] - 24s 284ms/step - loss: 0.1212 - accuracy: 0.8379 - val_loss: 0.5890 - val_accuracy: 0.6377\n",
      "Epoch 19/20\n",
      "84/84 [==============================] - 24s 283ms/step - loss: 0.1089 - accuracy: 0.8439 - val_loss: 0.5900 - val_accuracy: 0.6272\n",
      "Epoch 20/20\n",
      "84/84 [==============================] - 24s 288ms/step - loss: 0.0958 - accuracy: 0.8522 - val_loss: 0.6038 - val_accuracy: 0.6317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x129ef0cfbb0>"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "hypermodel.fit(trainX, trainY, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "e9bfc13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 2s 57ms/step - loss: 0.6186 - accuracy: 0.6311\n",
      "[test loss, test accuracy]: [0.6186283826828003, 0.6311377286911011]\n"
     ]
    }
   ],
   "source": [
    "eval_result = hypermodel.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d6ff7",
   "metadata": {},
   "source": [
    "### Tuned CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "8e6a6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = big_data[[\"Electronic\", \"Experimental\", \"Folk\", \"Hip-Hop\", \"International\", \"Punk\", \"Rock\"]]\n",
    "X = big_data[[\"mfcc\", \"chroma\", \"spectral\"]]\n",
    "mfcc = np.array(X['mfcc'].values.tolist())\n",
    "mfcc = np.reshape(mfcc, (mfcc.shape[0], mfcc.shape[1], mfcc.shape[2], 1))\n",
    "trainX, testX, trainY, testY = train_test_split(mfcc, Y, stratify=Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "1cadda5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(10, 469, 1)))\n",
    "    \n",
    "    hp_filters0 = hp.Int('filters0', min_value=4, max_value=16, step=4)\n",
    "    \n",
    "    hp_kernal0 = hp.Int('kernal0', min_value=2, max_value=4, step=1)\n",
    "    \n",
    "    hp_pool0 = hp.Int(\"pool0\", min_value=2, max_value=4, step=1)\n",
    "    \n",
    "    hp_drop = hp.Float(\"drop\", min_value=0, max_value=0.5, step=0.1)\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(filters=hp_filters0, kernel_size=(hp_kernal0, 1), activation=\"relu\"))\n",
    "    \n",
    "    model.add(keras.layers.MaxPool2D(pool_size=hp_pool0))\n",
    "    \n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(hp_drop))\n",
    "\n",
    "    model.add(keras.layers.Dense(7))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.CategoricalHinge(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "a9307b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 Complete [00h 00m 15s]\n",
      "val_accuracy: 0.7020958065986633\n",
      "\n",
      "Best val_accuracy So Far: 0.7305389046669006\n",
      "Total elapsed time: 00h 05m 16s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=50,\n",
    "                     factor=3)\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(trainX, trainY, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "56a921a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 20.8254 - accuracy: 0.2657 - val_loss: 8.5675 - val_accuracy: 0.4192\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 15.3159 - accuracy: 0.3469 - val_loss: 5.7459 - val_accuracy: 0.5554\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 12.7669 - accuracy: 0.4180 - val_loss: 4.5999 - val_accuracy: 0.6048\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 11.0383 - accuracy: 0.4416 - val_loss: 4.9716 - val_accuracy: 0.5973\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 9.6605 - accuracy: 0.4966 - val_loss: 4.1002 - val_accuracy: 0.6362\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 9.3948 - accuracy: 0.5094 - val_loss: 4.6834 - val_accuracy: 0.6317\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 8.4370 - accuracy: 0.5371 - val_loss: 3.8194 - val_accuracy: 0.6662\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 7.3506 - accuracy: 0.5584 - val_loss: 3.6767 - val_accuracy: 0.6557\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 6.6733 - accuracy: 0.5704 - val_loss: 3.5400 - val_accuracy: 0.6766\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 6.5549 - accuracy: 0.5861 - val_loss: 3.5905 - val_accuracy: 0.6692\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 5.8696 - accuracy: 0.6040 - val_loss: 3.5280 - val_accuracy: 0.6916\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 5.8459 - accuracy: 0.6119 - val_loss: 3.4907 - val_accuracy: 0.6811\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 5.5108 - accuracy: 0.6198 - val_loss: 3.5026 - val_accuracy: 0.6692\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 5.2717 - accuracy: 0.6239 - val_loss: 3.4158 - val_accuracy: 0.6961\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 4.3976 - accuracy: 0.6441 - val_loss: 3.2066 - val_accuracy: 0.6841\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 4.8856 - accuracy: 0.6332 - val_loss: 3.8785 - val_accuracy: 0.6737\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 4.2487 - accuracy: 0.6557 - val_loss: 3.4756 - val_accuracy: 0.6916\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 3.9997 - accuracy: 0.6557 - val_loss: 3.0500 - val_accuracy: 0.7006\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 4.0698 - accuracy: 0.6490 - val_loss: 3.4954 - val_accuracy: 0.7006\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 3.7994 - accuracy: 0.6692 - val_loss: 3.0612 - val_accuracy: 0.6961\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 3.7089 - accuracy: 0.6669 - val_loss: 2.8963 - val_accuracy: 0.6991\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 3.5116 - accuracy: 0.6684 - val_loss: 3.0020 - val_accuracy: 0.6961\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 3.1133 - accuracy: 0.6860 - val_loss: 3.6262 - val_accuracy: 0.6751\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 3.2479 - accuracy: 0.6714 - val_loss: 3.1400 - val_accuracy: 0.6991\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 3.1794 - accuracy: 0.6759 - val_loss: 2.9727 - val_accuracy: 0.7006\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.9450 - accuracy: 0.6811 - val_loss: 3.3145 - val_accuracy: 0.6766\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.7068 - accuracy: 0.6924 - val_loss: 2.9317 - val_accuracy: 0.6946\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.6156 - accuracy: 0.7006 - val_loss: 3.0001 - val_accuracy: 0.7096\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.4965 - accuracy: 0.6965 - val_loss: 2.9655 - val_accuracy: 0.7036\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.5228 - accuracy: 0.6987 - val_loss: 3.0019 - val_accuracy: 0.6991\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.3498 - accuracy: 0.6950 - val_loss: 2.9482 - val_accuracy: 0.6961\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.3985 - accuracy: 0.7036 - val_loss: 2.7702 - val_accuracy: 0.7021\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.0056 - accuracy: 0.7204 - val_loss: 2.8417 - val_accuracy: 0.7006\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.0845 - accuracy: 0.7182 - val_loss: 2.6754 - val_accuracy: 0.7141\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.1001 - accuracy: 0.7073 - val_loss: 2.8963 - val_accuracy: 0.6901\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.8921 - accuracy: 0.7204 - val_loss: 2.8051 - val_accuracy: 0.7021\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.7947 - accuracy: 0.7350 - val_loss: 2.7450 - val_accuracy: 0.7021\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.9148 - accuracy: 0.7219 - val_loss: 2.8686 - val_accuracy: 0.6991\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.7471 - accuracy: 0.7335 - val_loss: 2.6434 - val_accuracy: 0.7096\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.6496 - accuracy: 0.7388 - val_loss: 2.5504 - val_accuracy: 0.7126\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.6887 - accuracy: 0.7388 - val_loss: 2.6371 - val_accuracy: 0.6961\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.4926 - accuracy: 0.7358 - val_loss: 2.6125 - val_accuracy: 0.6946\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.5473 - accuracy: 0.7429 - val_loss: 3.2665 - val_accuracy: 0.6826\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.5520 - accuracy: 0.7380 - val_loss: 2.7705 - val_accuracy: 0.6961\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.4366 - accuracy: 0.7347 - val_loss: 2.7316 - val_accuracy: 0.7036\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.2736 - accuracy: 0.7530 - val_loss: 2.7621 - val_accuracy: 0.6916\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.2931 - accuracy: 0.7575 - val_loss: 2.5373 - val_accuracy: 0.7006\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.2343 - accuracy: 0.7552 - val_loss: 2.5924 - val_accuracy: 0.7051\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.2216 - accuracy: 0.7507 - val_loss: 2.4876 - val_accuracy: 0.6991\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.2079 - accuracy: 0.7507 - val_loss: 2.5880 - val_accuracy: 0.7051\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.0198 - accuracy: 0.7594 - val_loss: 2.7890 - val_accuracy: 0.7006\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.1683 - accuracy: 0.7590 - val_loss: 2.4951 - val_accuracy: 0.7081\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.9879 - accuracy: 0.7676 - val_loss: 2.7308 - val_accuracy: 0.7051\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.0960 - accuracy: 0.7672 - val_loss: 2.4243 - val_accuracy: 0.7111\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.0116 - accuracy: 0.7638 - val_loss: 2.5525 - val_accuracy: 0.6931\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.9327 - accuracy: 0.7638 - val_loss: 2.6795 - val_accuracy: 0.7141\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.9251 - accuracy: 0.7706 - val_loss: 2.4712 - val_accuracy: 0.7066\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.8650 - accuracy: 0.7784 - val_loss: 2.5585 - val_accuracy: 0.6931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.8341 - accuracy: 0.7803 - val_loss: 2.5301 - val_accuracy: 0.6931\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.8358 - accuracy: 0.7803 - val_loss: 2.7643 - val_accuracy: 0.6946\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.8801 - accuracy: 0.7721 - val_loss: 2.5973 - val_accuracy: 0.6916\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.8789 - accuracy: 0.7653 - val_loss: 2.7301 - val_accuracy: 0.6946\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.8080 - accuracy: 0.7751 - val_loss: 2.5423 - val_accuracy: 0.6961\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.7479 - accuracy: 0.7822 - val_loss: 2.3985 - val_accuracy: 0.7036\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.7874 - accuracy: 0.7698 - val_loss: 2.3809 - val_accuracy: 0.7036\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6542 - accuracy: 0.7930 - val_loss: 2.5393 - val_accuracy: 0.7036\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.8947 - accuracy: 0.7792 - val_loss: 2.5465 - val_accuracy: 0.6976\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6804 - accuracy: 0.7912 - val_loss: 2.5872 - val_accuracy: 0.7036\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.7150 - accuracy: 0.7822 - val_loss: 2.5977 - val_accuracy: 0.7021\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.7371 - accuracy: 0.7859 - val_loss: 2.6832 - val_accuracy: 0.6871\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5922 - accuracy: 0.7953 - val_loss: 2.5269 - val_accuracy: 0.7066\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5848 - accuracy: 0.7938 - val_loss: 2.5865 - val_accuracy: 0.6931\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6196 - accuracy: 0.7919 - val_loss: 2.3991 - val_accuracy: 0.7021\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6183 - accuracy: 0.7953 - val_loss: 2.4395 - val_accuracy: 0.7081\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5593 - accuracy: 0.7998 - val_loss: 2.6225 - val_accuracy: 0.6991\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6150 - accuracy: 0.7960 - val_loss: 2.4295 - val_accuracy: 0.6991\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5720 - accuracy: 0.7968 - val_loss: 2.6933 - val_accuracy: 0.7066\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4788 - accuracy: 0.8028 - val_loss: 2.4252 - val_accuracy: 0.7006\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5193 - accuracy: 0.8009 - val_loss: 2.4471 - val_accuracy: 0.7036\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.6058 - accuracy: 0.7953 - val_loss: 2.4988 - val_accuracy: 0.7051\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4693 - accuracy: 0.8039 - val_loss: 2.4794 - val_accuracy: 0.7096\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5002 - accuracy: 0.8009 - val_loss: 2.6432 - val_accuracy: 0.6946\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4840 - accuracy: 0.8110 - val_loss: 2.4018 - val_accuracy: 0.6991\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3646 - accuracy: 0.8132 - val_loss: 2.4746 - val_accuracy: 0.7021\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5517 - accuracy: 0.7934 - val_loss: 2.3908 - val_accuracy: 0.6991\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4418 - accuracy: 0.8132 - val_loss: 2.4530 - val_accuracy: 0.7006\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4981 - accuracy: 0.8088 - val_loss: 2.3497 - val_accuracy: 0.6991\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4201 - accuracy: 0.8200 - val_loss: 2.3418 - val_accuracy: 0.7051\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5201 - accuracy: 0.8020 - val_loss: 2.6108 - val_accuracy: 0.7096\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4209 - accuracy: 0.8114 - val_loss: 2.4156 - val_accuracy: 0.7141\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3783 - accuracy: 0.8174 - val_loss: 2.5507 - val_accuracy: 0.7066\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4100 - accuracy: 0.8140 - val_loss: 2.5376 - val_accuracy: 0.6931\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3609 - accuracy: 0.8200 - val_loss: 2.5894 - val_accuracy: 0.7036\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3331 - accuracy: 0.8204 - val_loss: 2.4221 - val_accuracy: 0.6991\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2997 - accuracy: 0.8245 - val_loss: 2.5198 - val_accuracy: 0.7006\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.4075 - accuracy: 0.8181 - val_loss: 2.4604 - val_accuracy: 0.6856\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3588 - accuracy: 0.8215 - val_loss: 2.4951 - val_accuracy: 0.6961\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3468 - accuracy: 0.8219 - val_loss: 2.3495 - val_accuracy: 0.7096\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.3221 - accuracy: 0.8215 - val_loss: 2.4158 - val_accuracy: 0.6961\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2735 - accuracy: 0.8286 - val_loss: 2.4775 - val_accuracy: 0.7036\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(trainX, trainY, epochs=100, validation_split=0.2)\n",
    "\n",
    "val_acc = history.history['val_accuracy']\n",
    "best_epoch = val_acc.index(max(val_acc))+1\n",
    "print(best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "4102fe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/34\n",
      "84/84 [==============================] - 1s 5ms/step - loss: 22.3485 - accuracy: 0.2526 - val_loss: 6.4684 - val_accuracy: 0.4371\n",
      "Epoch 2/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 15.2386 - accuracy: 0.3428 - val_loss: 4.9371 - val_accuracy: 0.5449\n",
      "Epoch 3/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 12.9502 - accuracy: 0.4004 - val_loss: 4.8679 - val_accuracy: 0.5958\n",
      "Epoch 4/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 11.2782 - accuracy: 0.4484 - val_loss: 4.3266 - val_accuracy: 0.6243\n",
      "Epoch 5/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 9.8335 - accuracy: 0.4772 - val_loss: 4.4730 - val_accuracy: 0.6198\n",
      "Epoch 6/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 9.7659 - accuracy: 0.5075 - val_loss: 3.9507 - val_accuracy: 0.6272\n",
      "Epoch 7/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 8.3680 - accuracy: 0.5258 - val_loss: 3.8785 - val_accuracy: 0.6572\n",
      "Epoch 8/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 7.7730 - accuracy: 0.5621 - val_loss: 3.4566 - val_accuracy: 0.6632\n",
      "Epoch 9/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 7.0243 - accuracy: 0.5662 - val_loss: 3.2152 - val_accuracy: 0.6826\n",
      "Epoch 10/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 6.4927 - accuracy: 0.5808 - val_loss: 3.1973 - val_accuracy: 0.6796\n",
      "Epoch 11/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 6.0404 - accuracy: 0.5891 - val_loss: 3.5996 - val_accuracy: 0.6707\n",
      "Epoch 12/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 5.7551 - accuracy: 0.6044 - val_loss: 3.4740 - val_accuracy: 0.6901\n",
      "Epoch 13/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 5.6372 - accuracy: 0.6055 - val_loss: 3.2470 - val_accuracy: 0.6856\n",
      "Epoch 14/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 4.8397 - accuracy: 0.6321 - val_loss: 3.2707 - val_accuracy: 0.6886\n",
      "Epoch 15/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 4.7107 - accuracy: 0.6291 - val_loss: 3.1523 - val_accuracy: 0.6886\n",
      "Epoch 16/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 4.4056 - accuracy: 0.6418 - val_loss: 3.3171 - val_accuracy: 0.6841\n",
      "Epoch 17/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 4.1653 - accuracy: 0.6587 - val_loss: 3.1695 - val_accuracy: 0.6991\n",
      "Epoch 18/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 3.8826 - accuracy: 0.6621 - val_loss: 2.9082 - val_accuracy: 0.7021\n",
      "Epoch 19/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 4.1070 - accuracy: 0.6482 - val_loss: 2.9576 - val_accuracy: 0.6901\n",
      "Epoch 20/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 3.5806 - accuracy: 0.6632 - val_loss: 2.9038 - val_accuracy: 0.7036\n",
      "Epoch 21/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 3.7226 - accuracy: 0.6692 - val_loss: 2.8844 - val_accuracy: 0.7066\n",
      "Epoch 22/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 3.1758 - accuracy: 0.6804 - val_loss: 3.1540 - val_accuracy: 0.6946\n",
      "Epoch 23/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 3.0206 - accuracy: 0.6808 - val_loss: 2.7989 - val_accuracy: 0.6991\n",
      "Epoch 24/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.9599 - accuracy: 0.6845 - val_loss: 3.0046 - val_accuracy: 0.6946\n",
      "Epoch 25/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.8859 - accuracy: 0.6879 - val_loss: 2.9144 - val_accuracy: 0.6826\n",
      "Epoch 26/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.9668 - accuracy: 0.6882 - val_loss: 3.1599 - val_accuracy: 0.6976\n",
      "Epoch 27/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.7870 - accuracy: 0.6976 - val_loss: 3.1941 - val_accuracy: 0.6916\n",
      "Epoch 28/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.5208 - accuracy: 0.6954 - val_loss: 2.9344 - val_accuracy: 0.7006\n",
      "Epoch 29/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.4243 - accuracy: 0.7066 - val_loss: 2.9637 - val_accuracy: 0.7051\n",
      "Epoch 30/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.4199 - accuracy: 0.7081 - val_loss: 2.9846 - val_accuracy: 0.6811\n",
      "Epoch 31/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.1899 - accuracy: 0.7122 - val_loss: 2.7728 - val_accuracy: 0.6961\n",
      "Epoch 32/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.8905 - accuracy: 0.7317 - val_loss: 2.7908 - val_accuracy: 0.7006\n",
      "Epoch 33/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 2.0872 - accuracy: 0.7085 - val_loss: 2.7601 - val_accuracy: 0.7021\n",
      "Epoch 34/34\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 1.9403 - accuracy: 0.7246 - val_loss: 2.7706 - val_accuracy: 0.6931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x129f5a4de10>"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "hypermodel.fit(trainX, trainY, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "cce7e1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 2ms/step - loss: 3.0016 - accuracy: 0.6695\n",
      "[test loss, test accuracy]: [3.001553535461426, 0.6694610714912415]\n"
     ]
    }
   ],
   "source": [
    "eval_result = hypermodel.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb88ba5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6517eeba",
   "metadata": {},
   "source": [
    "### Trying some more models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "75ea6e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hps.get('filters0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab8f4d7",
   "metadata": {},
   "source": [
    "##### This was maxed, lets try increasing the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "c94f1ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hps.get('kernal0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "033c5e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hps.get(\"pool0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "cb710258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hps.get(\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "df78b277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hps.get(\"learning_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "399d25e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "098db067",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = trainX.shape[1]\n",
    "features = trainX.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "218f89ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "f93ccff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(10, 469, 1)))\n",
    "    \n",
    "    hp_kernal0 = hp.Int('kernal0', min_value=4, max_value=8, step=1)\n",
    "    \n",
    "    #hp_lstm = hp.Int(\"lstm\", min_value=64, max_value=256, step=64)\n",
    "    #hp_drop0 = hp.Float(\"drop0\", min_value=0, max_value=0.5, step=0.1)\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(filters=12, kernel_size=(5, 1), activation=\"relu\"))\n",
    "    \n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    \n",
    "    model.add(keras.layers.Flatten())\n",
    "    #model.add(tf.keras.layers.Reshape(target_shape=(18, 468)))\n",
    "    #model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hp_lstm, dropout=hp_drop0)))\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "    model.add(keras.layers.Dense(7))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                loss=keras.losses.CategoricalHinge(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "821d0a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 84 Complete [00h 00m 18s]\n",
      "val_accuracy: 0.7290419340133667\n",
      "\n",
      "Best val_accuracy So Far: 0.735029935836792\n",
      "Total elapsed time: 00h 05m 09s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=50,\n",
    "                     factor=3)\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(trainX, trainY, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "2652e9f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hps.get(\"kernal0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "2e87fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(10, 469, 1)))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(filters=12, kernel_size=(5, 1), activation=\"relu\"))\n",
    "    \n",
    "    model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "    \n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=128, step=32)\n",
    "    hp_units1 = hp.Int('units1', min_value=64, max_value=256, step=64)\n",
    "    hp_units2 = hp.Int('units2', min_value=64, max_value=256, step=64)\n",
    "    hp_units3 = hp.Int('units3', min_value=64, max_value=256, step=64)\n",
    "    hp_units4 = hp.Int('units4', min_value=64, max_value=256, step=64)\n",
    "    \n",
    "    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "    model.add(keras.layers.Dense(units=hp_units1, activation='relu'))\n",
    "    model.add(keras.layers.Dense(units=hp_units2, activation='relu'))\n",
    "    model.add(keras.layers.Dense(units=hp_units3, activation='relu'))\n",
    "    model.add(keras.layers.Dense(units=hp_units4, activation='relu'))\n",
    "    \n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "\n",
    "    model.add(keras.layers.Dense(7))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                loss=keras.losses.CategoricalHinge(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "7cb5bc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 Complete [00h 00m 10s]\n",
      "val_accuracy: 0.4940119683742523\n",
      "\n",
      "Best val_accuracy So Far: 0.7215569019317627\n",
      "Total elapsed time: 00h 06m 53s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=50,\n",
    "                     factor=3)\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(trainX, trainY, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b40ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "1144bb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 2.1123 - accuracy: 0.2631 - val_loss: 0.9532 - val_accuracy: 0.4611\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 1.0690 - accuracy: 0.4207 - val_loss: 0.7598 - val_accuracy: 0.5479\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.8449 - accuracy: 0.5161 - val_loss: 0.7091 - val_accuracy: 0.5464\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.7077 - accuracy: 0.5865 - val_loss: 0.5461 - val_accuracy: 0.6452\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.5740 - accuracy: 0.6396 - val_loss: 0.5363 - val_accuracy: 0.6617\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.5234 - accuracy: 0.6714 - val_loss: 0.4739 - val_accuracy: 0.6707\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.5164 - accuracy: 0.6699 - val_loss: 0.4393 - val_accuracy: 0.6737\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.4156 - accuracy: 0.7058 - val_loss: 0.4122 - val_accuracy: 0.7036\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3868 - accuracy: 0.7167 - val_loss: 0.4423 - val_accuracy: 0.6662\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.3584 - accuracy: 0.7234 - val_loss: 0.4369 - val_accuracy: 0.6662\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3523 - accuracy: 0.7376 - val_loss: 0.4056 - val_accuracy: 0.7006\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3102 - accuracy: 0.7496 - val_loss: 0.4357 - val_accuracy: 0.7006\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3026 - accuracy: 0.7530 - val_loss: 0.4135 - val_accuracy: 0.7036\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2838 - accuracy: 0.7522 - val_loss: 0.3995 - val_accuracy: 0.6991\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.2827 - accuracy: 0.7575 - val_loss: 0.5037 - val_accuracy: 0.6766\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.3011 - accuracy: 0.7481 - val_loss: 0.4576 - val_accuracy: 0.7006\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2402 - accuracy: 0.7792 - val_loss: 0.4328 - val_accuracy: 0.7036\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2252 - accuracy: 0.7758 - val_loss: 0.4362 - val_accuracy: 0.7006\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.2244 - accuracy: 0.7811 - val_loss: 0.4000 - val_accuracy: 0.7156\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2006 - accuracy: 0.7919 - val_loss: 0.4082 - val_accuracy: 0.7021\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1994 - accuracy: 0.7975 - val_loss: 0.3983 - val_accuracy: 0.7066\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1798 - accuracy: 0.8043 - val_loss: 0.4262 - val_accuracy: 0.6946\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1871 - accuracy: 0.8013 - val_loss: 0.3770 - val_accuracy: 0.7096\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1663 - accuracy: 0.8080 - val_loss: 0.4043 - val_accuracy: 0.7081\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1680 - accuracy: 0.8076 - val_loss: 0.3956 - val_accuracy: 0.7081\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1621 - accuracy: 0.8076 - val_loss: 0.3948 - val_accuracy: 0.7156\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1339 - accuracy: 0.8215 - val_loss: 0.4190 - val_accuracy: 0.7006\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1479 - accuracy: 0.8185 - val_loss: 0.4688 - val_accuracy: 0.7006\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1354 - accuracy: 0.8174 - val_loss: 0.4454 - val_accuracy: 0.7156\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1404 - accuracy: 0.8230 - val_loss: 0.4374 - val_accuracy: 0.6961\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.8293 - val_loss: 0.3970 - val_accuracy: 0.7141\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1177 - accuracy: 0.8278 - val_loss: 0.4763 - val_accuracy: 0.7141\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1063 - accuracy: 0.8327 - val_loss: 0.4150 - val_accuracy: 0.7201\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1025 - accuracy: 0.8316 - val_loss: 0.4143 - val_accuracy: 0.7201\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0916 - accuracy: 0.8376 - val_loss: 0.4662 - val_accuracy: 0.7006\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0951 - accuracy: 0.8379 - val_loss: 0.4358 - val_accuracy: 0.7231\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0960 - accuracy: 0.8394 - val_loss: 0.4689 - val_accuracy: 0.6961\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0806 - accuracy: 0.8447 - val_loss: 0.4238 - val_accuracy: 0.7186\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0690 - accuracy: 0.8469 - val_loss: 0.4569 - val_accuracy: 0.7111\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0737 - accuracy: 0.8466 - val_loss: 0.4221 - val_accuracy: 0.7275\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0810 - accuracy: 0.8424 - val_loss: 0.4453 - val_accuracy: 0.7141\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0972 - accuracy: 0.8342 - val_loss: 0.4855 - val_accuracy: 0.7036\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0858 - accuracy: 0.8379 - val_loss: 0.4531 - val_accuracy: 0.7081\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0632 - accuracy: 0.8477 - val_loss: 0.5022 - val_accuracy: 0.7096\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0794 - accuracy: 0.8436 - val_loss: 0.5243 - val_accuracy: 0.6991\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0722 - accuracy: 0.8496 - val_loss: 0.5577 - val_accuracy: 0.7051\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0718 - accuracy: 0.8451 - val_loss: 0.4965 - val_accuracy: 0.7096\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0723 - accuracy: 0.8451 - val_loss: 0.5412 - val_accuracy: 0.7066\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0565 - accuracy: 0.8529 - val_loss: 0.4993 - val_accuracy: 0.7051\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0516 - accuracy: 0.8533 - val_loss: 0.4851 - val_accuracy: 0.7126\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0594 - accuracy: 0.8514 - val_loss: 0.5086 - val_accuracy: 0.7081\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0438 - accuracy: 0.8567 - val_loss: 0.4678 - val_accuracy: 0.7081\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0410 - accuracy: 0.8570 - val_loss: 0.4704 - val_accuracy: 0.7066\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0452 - accuracy: 0.8574 - val_loss: 0.5255 - val_accuracy: 0.7171\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0353 - accuracy: 0.8615 - val_loss: 0.5571 - val_accuracy: 0.7051\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0495 - accuracy: 0.8529 - val_loss: 0.5106 - val_accuracy: 0.7156\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0299 - accuracy: 0.8634 - val_loss: 0.5288 - val_accuracy: 0.7126\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0605 - accuracy: 0.8499 - val_loss: 0.5693 - val_accuracy: 0.7006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0873 - accuracy: 0.8436 - val_loss: 0.5645 - val_accuracy: 0.6976\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0492 - accuracy: 0.8533 - val_loss: 0.5572 - val_accuracy: 0.7156\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0405 - accuracy: 0.8608 - val_loss: 0.5543 - val_accuracy: 0.7156\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0326 - accuracy: 0.8619 - val_loss: 0.5895 - val_accuracy: 0.6961\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0288 - accuracy: 0.8656 - val_loss: 0.5333 - val_accuracy: 0.7081\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0418 - accuracy: 0.8589 - val_loss: 0.5653 - val_accuracy: 0.7081\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0243 - accuracy: 0.8653 - val_loss: 0.5857 - val_accuracy: 0.6991\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 0.8641 - val_loss: 0.5477 - val_accuracy: 0.7260\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0219 - accuracy: 0.8671 - val_loss: 0.6592 - val_accuracy: 0.6931\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0399 - accuracy: 0.8578 - val_loss: 0.5405 - val_accuracy: 0.7111\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0264 - accuracy: 0.8641 - val_loss: 0.5726 - val_accuracy: 0.6991\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0207 - accuracy: 0.8664 - val_loss: 0.5882 - val_accuracy: 0.7201\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0347 - accuracy: 0.8626 - val_loss: 0.6578 - val_accuracy: 0.6991\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0514 - accuracy: 0.8548 - val_loss: 0.6231 - val_accuracy: 0.7111\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0430 - accuracy: 0.8582 - val_loss: 0.5315 - val_accuracy: 0.7141\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0230 - accuracy: 0.8653 - val_loss: 0.5177 - val_accuracy: 0.7275\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0376 - accuracy: 0.8600 - val_loss: 0.5689 - val_accuracy: 0.7246\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0244 - accuracy: 0.8653 - val_loss: 0.6280 - val_accuracy: 0.7021\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0477 - accuracy: 0.8593 - val_loss: 0.6457 - val_accuracy: 0.7096\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0264 - accuracy: 0.8645 - val_loss: 0.6404 - val_accuracy: 0.7156\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0134 - accuracy: 0.8686 - val_loss: 0.6169 - val_accuracy: 0.7051\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0083 - accuracy: 0.8731 - val_loss: 0.7789 - val_accuracy: 0.7066\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0274 - accuracy: 0.8630 - val_loss: 0.6566 - val_accuracy: 0.7126\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0365 - accuracy: 0.8623 - val_loss: 0.5637 - val_accuracy: 0.7141\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0638 - accuracy: 0.8559 - val_loss: 0.5885 - val_accuracy: 0.7051\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0355 - accuracy: 0.8634 - val_loss: 0.6842 - val_accuracy: 0.7171\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0216 - accuracy: 0.8668 - val_loss: 0.5637 - val_accuracy: 0.7156\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0194 - accuracy: 0.8686 - val_loss: 0.6144 - val_accuracy: 0.7111\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0264 - accuracy: 0.8649 - val_loss: 0.7412 - val_accuracy: 0.7156\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0327 - accuracy: 0.8619 - val_loss: 0.6335 - val_accuracy: 0.7216\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.8713 - val_loss: 0.5321 - val_accuracy: 0.7231\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0209 - accuracy: 0.8671 - val_loss: 0.5396 - val_accuracy: 0.7216\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0268 - accuracy: 0.8656 - val_loss: 0.5955 - val_accuracy: 0.7081\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0335 - accuracy: 0.8638 - val_loss: 0.6502 - val_accuracy: 0.7096\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0312 - accuracy: 0.8649 - val_loss: 0.7152 - val_accuracy: 0.6991\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0165 - accuracy: 0.8679 - val_loss: 0.6271 - val_accuracy: 0.7171\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0031 - accuracy: 0.8739 - val_loss: 0.6239 - val_accuracy: 0.7126\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 0.8728 - val_loss: 0.6583 - val_accuracy: 0.6991\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0143 - accuracy: 0.8686 - val_loss: 0.6113 - val_accuracy: 0.7275\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0119 - accuracy: 0.8705 - val_loss: 0.6704 - val_accuracy: 0.7171\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0213 - accuracy: 0.8671 - val_loss: 0.6551 - val_accuracy: 0.7141\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0079 - accuracy: 0.8701 - val_loss: 0.6414 - val_accuracy: 0.7066\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(trainX, trainY, epochs=100, validation_split=0.2)\n",
    "\n",
    "val_acc = history.history['val_accuracy']\n",
    "best_epoch = val_acc.index(max(val_acc))+1\n",
    "print(best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "7da39515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.6358 - accuracy: 0.2814 - val_loss: 0.9049 - val_accuracy: 0.4237\n",
      "Epoch 2/40\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.9496 - accuracy: 0.3885 - val_loss: 0.8358 - val_accuracy: 0.4371\n",
      "Epoch 3/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.7926 - accuracy: 0.4865 - val_loss: 0.6438 - val_accuracy: 0.5689\n",
      "Epoch 4/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.6237 - accuracy: 0.5767 - val_loss: 0.5448 - val_accuracy: 0.5868\n",
      "Epoch 5/40\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.5369 - accuracy: 0.6063 - val_loss: 0.5351 - val_accuracy: 0.5958\n",
      "Epoch 6/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4904 - accuracy: 0.6284 - val_loss: 0.4886 - val_accuracy: 0.6063\n",
      "Epoch 7/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4383 - accuracy: 0.6415 - val_loss: 0.4592 - val_accuracy: 0.6392\n",
      "Epoch 8/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.4151 - accuracy: 0.6684 - val_loss: 0.4625 - val_accuracy: 0.6437\n",
      "Epoch 9/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3936 - accuracy: 0.6875 - val_loss: 0.4981 - val_accuracy: 0.6392\n",
      "Epoch 10/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3699 - accuracy: 0.6897 - val_loss: 0.4355 - val_accuracy: 0.6617\n",
      "Epoch 11/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3642 - accuracy: 0.6931 - val_loss: 0.4514 - val_accuracy: 0.6347\n",
      "Epoch 12/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3191 - accuracy: 0.7107 - val_loss: 0.4262 - val_accuracy: 0.6692\n",
      "Epoch 13/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.3165 - accuracy: 0.7025 - val_loss: 0.4158 - val_accuracy: 0.6422\n",
      "Epoch 14/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2899 - accuracy: 0.7129 - val_loss: 0.4149 - val_accuracy: 0.6647\n",
      "Epoch 15/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2825 - accuracy: 0.7197 - val_loss: 0.4409 - val_accuracy: 0.6766\n",
      "Epoch 16/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2788 - accuracy: 0.7223 - val_loss: 0.3866 - val_accuracy: 0.6781\n",
      "Epoch 17/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2507 - accuracy: 0.7373 - val_loss: 0.4333 - val_accuracy: 0.6542\n",
      "Epoch 18/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2525 - accuracy: 0.7268 - val_loss: 0.4050 - val_accuracy: 0.6781\n",
      "Epoch 19/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2309 - accuracy: 0.7414 - val_loss: 0.4136 - val_accuracy: 0.6692\n",
      "Epoch 20/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2310 - accuracy: 0.7440 - val_loss: 0.4788 - val_accuracy: 0.6362\n",
      "Epoch 21/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2313 - accuracy: 0.7444 - val_loss: 0.4033 - val_accuracy: 0.6931\n",
      "Epoch 22/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2006 - accuracy: 0.7650 - val_loss: 0.3961 - val_accuracy: 0.6886\n",
      "Epoch 23/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1996 - accuracy: 0.7601 - val_loss: 0.3848 - val_accuracy: 0.6692\n",
      "Epoch 24/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1902 - accuracy: 0.7672 - val_loss: 0.4555 - val_accuracy: 0.6542\n",
      "Epoch 25/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.2005 - accuracy: 0.7646 - val_loss: 0.4922 - val_accuracy: 0.6737\n",
      "Epoch 26/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1925 - accuracy: 0.7635 - val_loss: 0.4519 - val_accuracy: 0.6766\n",
      "Epoch 27/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1857 - accuracy: 0.7661 - val_loss: 0.4228 - val_accuracy: 0.6751\n",
      "Epoch 28/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1668 - accuracy: 0.7762 - val_loss: 0.4550 - val_accuracy: 0.6617\n",
      "Epoch 29/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1599 - accuracy: 0.7781 - val_loss: 0.4630 - val_accuracy: 0.7051\n",
      "Epoch 30/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1774 - accuracy: 0.7781 - val_loss: 0.4514 - val_accuracy: 0.6946\n",
      "Epoch 31/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1724 - accuracy: 0.7736 - val_loss: 0.4353 - val_accuracy: 0.6991\n",
      "Epoch 32/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1447 - accuracy: 0.7893 - val_loss: 0.3917 - val_accuracy: 0.6841\n",
      "Epoch 33/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1491 - accuracy: 0.7852 - val_loss: 0.3839 - val_accuracy: 0.6961\n",
      "Epoch 34/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1256 - accuracy: 0.7927 - val_loss: 0.4061 - val_accuracy: 0.6916\n",
      "Epoch 35/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1164 - accuracy: 0.7938 - val_loss: 0.4017 - val_accuracy: 0.7051\n",
      "Epoch 36/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1239 - accuracy: 0.7957 - val_loss: 0.4248 - val_accuracy: 0.6991\n",
      "Epoch 37/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1322 - accuracy: 0.7930 - val_loss: 0.4244 - val_accuracy: 0.7081\n",
      "Epoch 38/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1209 - accuracy: 0.7990 - val_loss: 0.4001 - val_accuracy: 0.7111\n",
      "Epoch 39/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1109 - accuracy: 0.8046 - val_loss: 0.3891 - val_accuracy: 0.7036\n",
      "Epoch 40/40\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1225 - accuracy: 0.7979 - val_loss: 0.4372 - val_accuracy: 0.6976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x129cb758040>"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "hypermodel.fit(trainX, trainY, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "0421648f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 2ms/step - loss: 0.5507 - accuracy: 0.6623\n",
      "[test loss, test accuracy]: [0.5506706237792969, 0.6622754335403442]\n"
     ]
    }
   ],
   "source": [
    "eval_result = hypermodel.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438b0bb5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ddbc0c4",
   "metadata": {},
   "source": [
    "### Best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "id": "0362566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(1, 1, 469, 1)))\n",
    "    \n",
    "    hp_units = hp.Int('units', min_value=32, max_value=128, step=32)\n",
    "    hp_units1 = hp.Int('units1', min_value=64, max_value=256, step=64)\n",
    "    hp_units2 = hp.Int('units2', min_value=64, max_value=256, step=64)\n",
    "    hp_units3 = hp.Int('units3', min_value=64, max_value=256, step=64)\n",
    "    hp_units4 = hp.Int('units4', min_value=64, max_value=256, step=64)\n",
    "    \n",
    "    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "    model.add(keras.layers.Dense(units=hp_units1, activation='relu'))\n",
    "    model.add(keras.layers.Dense(units=hp_units2, activation='relu'))\n",
    "    model.add(keras.layers.Dense(units=hp_units3, activation='relu'))\n",
    "    model.add(keras.layers.Dense(units=hp_units4, activation='relu'))\n",
    "    \n",
    "    hp_drop = hp.Float('drop', min_value=0, max_value=0.5, step=0.1)\n",
    "    \n",
    "    model.add(tf.keras.layers.Dropout(rate=hp_drop),)\n",
    "    \n",
    "    model.add(keras.layers.Dense(7))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.CategoricalHinge(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "id": "8af7f265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_33 (Flatten)        (None, 4690)              0         \n",
      "                                                                 \n",
      " dense_163 (Dense)           (None, 96)                450336    \n",
      "                                                                 \n",
      " dense_164 (Dense)           (None, 64)                6208      \n",
      "                                                                 \n",
      " dense_165 (Dense)           (None, 192)               12480     \n",
      "                                                                 \n",
      " dense_166 (Dense)           (None, 192)               37056     \n",
      "                                                                 \n",
      " dense_167 (Dense)           (None, 64)                12352     \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_168 (Dense)           (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 518,887\n",
      "Trainable params: 518,887\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(1, 10, 469, 1)))\n",
    "\n",
    "model.add(keras.layers.Dense(units=fcbest_hps.get(\"units\"), activation='relu'))\n",
    "model.add(keras.layers.Dense(units=fcbest_hps.get(\"units1\"), activation='relu'))\n",
    "model.add(keras.layers.Dense(units=fcbest_hps.get(\"units2\"), activation='relu'))\n",
    "model.add(keras.layers.Dense(units=fcbest_hps.get(\"units3\"), activation='relu'))\n",
    "model.add(keras.layers.Dense(units=fcbest_hps.get(\"units4\"), activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(rate=fcbest_hps.get(\"drop\")),)\n",
    "    \n",
    "model.add(keras.layers.Dense(7))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=fcbest_hps.get(\"learning_rate\")),\n",
    "                loss=keras.losses.CategoricalHinge(),\n",
    "                metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "id": "88bdb2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_34 (Flatten)        (None, 469)               0         \n",
      "                                                                 \n",
      " dense_169 (Dense)           (None, 96)                45120     \n",
      "                                                                 \n",
      " dense_170 (Dense)           (None, 64)                6208      \n",
      "                                                                 \n",
      " dense_171 (Dense)           (None, 192)               12480     \n",
      "                                                                 \n",
      " dense_172 (Dense)           (None, 192)               37056     \n",
      "                                                                 \n",
      " dense_173 (Dense)           (None, 64)                12352     \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_174 (Dense)           (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 113,671\n",
      "Trainable params: 113,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "smodel = keras.Sequential()\n",
    "smodel.add(tf.keras.layers.Flatten(input_shape=(1, 469, 1)))\n",
    "\n",
    "smodel.add(keras.layers.Dense(units=fcbest_hps.get(\"units\"), activation='relu'))\n",
    "smodel.add(keras.layers.Dense(units=fcbest_hps.get(\"units1\"), activation='relu'))\n",
    "smodel.add(keras.layers.Dense(units=fcbest_hps.get(\"units2\"), activation='relu'))\n",
    "smodel.add(keras.layers.Dense(units=fcbest_hps.get(\"units3\"), activation='relu'))\n",
    "smodel.add(keras.layers.Dense(units=fcbest_hps.get(\"units4\"), activation='relu'))\n",
    "\n",
    "smodel.add(tf.keras.layers.Dropout(rate=fcbest_hps.get(\"drop\")),)\n",
    "    \n",
    "smodel.add(keras.layers.Dense(7))\n",
    "\n",
    "smodel.compile(optimizer=keras.optimizers.Adam(learning_rate=fcbest_hps.get(\"learning_rate\")),\n",
    "                loss=keras.losses.CategoricalHinge(),\n",
    "                metrics=['accuracy'])\n",
    "print(smodel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "id": "60bc90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = big_data[[\"Electronic\", \"Experimental\", \"Folk\", \"Hip-Hop\", \"International\", \"Punk\", \"Rock\"]]\n",
    "X = big_data[[\"zcross\", \"mfcc\", \"chroma\", \"spectral\"]]\n",
    "X.loc[:,'chroma'] = X['chroma'].apply(lambda x: x.reshape(-1,10,469,1))\n",
    "chroma = np.array(X['chroma'].tolist())\n",
    "\n",
    "X.loc[:,'mfcc'] = X['mfcc'].apply(lambda x: x.reshape(-1,10,469,1))\n",
    "mfcc= np.array(X['mfcc'].tolist())\n",
    "\n",
    "X.loc[:,'spectral'] = X['spectral'].apply(lambda x: x.reshape(-1,469,1))\n",
    "spectral= np.array(X['spectral'].tolist())\n",
    "\n",
    "X.loc[:,'zcross'] = X['zcross'].apply(lambda x: x.reshape(-1,469,1))\n",
    "zcross= np.array(X['zcross'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "id": "d0993e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "53/53 [==============================] - 1s 9ms/step - loss: 0.0181 - accuracy: 0.8677 - val_loss: 0.0276 - val_accuracy: 0.8647\n",
      "Epoch 2/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0291 - accuracy: 0.8632 - val_loss: 0.0132 - val_accuracy: 0.8695\n",
      "Epoch 3/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0187 - accuracy: 0.8671 - val_loss: 0.0215 - val_accuracy: 0.8647\n",
      "Epoch 4/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0479 - accuracy: 0.8560 - val_loss: 0.0314 - val_accuracy: 0.8623\n",
      "Epoch 5/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 0.8713 - val_loss: 0.0106 - val_accuracy: 0.8683\n",
      "Epoch 6/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0038 - accuracy: 0.8722 - val_loss: 0.0257 - val_accuracy: 0.8623\n",
      "Epoch 7/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0049 - accuracy: 0.8719 - val_loss: 0.0144 - val_accuracy: 0.8671\n",
      "Epoch 8/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0023 - accuracy: 0.8734 - val_loss: 0.0081 - val_accuracy: 0.8695\n",
      "Epoch 9/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.8731 - val_loss: 0.0099 - val_accuracy: 0.8695\n",
      "Epoch 10/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.8713 - val_loss: 0.0166 - val_accuracy: 0.8683\n",
      "Epoch 11/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 0.8719 - val_loss: 0.0100 - val_accuracy: 0.8695\n",
      "Epoch 12/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0010 - accuracy: 0.8731 - val_loss: 0.0143 - val_accuracy: 0.8659\n",
      "Epoch 13/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 0.8734 - val_loss: 0.0103 - val_accuracy: 0.8671\n",
      "Epoch 14/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.8710 - val_loss: 0.0095 - val_accuracy: 0.8683\n",
      "Epoch 15/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 8.9217e-04 - accuracy: 0.8734 - val_loss: 0.0121 - val_accuracy: 0.8671\n",
      "Epoch 16/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0011 - accuracy: 0.8728 - val_loss: 0.0079 - val_accuracy: 0.8695\n",
      "Epoch 17/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 3.9315e-04 - accuracy: 0.8734 - val_loss: 0.0103 - val_accuracy: 0.8695\n",
      "Epoch 18/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 6.3080e-04 - accuracy: 0.8734 - val_loss: 0.0115 - val_accuracy: 0.8683\n",
      "Epoch 19/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0164 - accuracy: 0.8674 - val_loss: 0.0229 - val_accuracy: 0.8647\n",
      "Epoch 20/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.8653 - val_loss: 0.2681 - val_accuracy: 0.7772\n",
      "Epoch 21/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0423 - accuracy: 0.8602 - val_loss: 0.0274 - val_accuracy: 0.8635\n",
      "Epoch 22/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0023 - accuracy: 0.8728 - val_loss: 0.0116 - val_accuracy: 0.8707\n",
      "Epoch 23/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.8725 - val_loss: 0.0158 - val_accuracy: 0.8683\n",
      "Epoch 24/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 7.4635e-04 - accuracy: 0.8734 - val_loss: 0.0145 - val_accuracy: 0.8671\n",
      "Epoch 25/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 9.9591e-04 - accuracy: 0.8734 - val_loss: 0.0124 - val_accuracy: 0.8683\n",
      "Epoch 26/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.4675e-04 - accuracy: 0.8734 - val_loss: 0.0112 - val_accuracy: 0.8683\n",
      "Epoch 27/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.7227e-04 - accuracy: 0.8734 - val_loss: 0.0114 - val_accuracy: 0.8683\n",
      "Epoch 28/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 9.6057e-04 - accuracy: 0.8731 - val_loss: 0.0139 - val_accuracy: 0.8683\n",
      "Epoch 29/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.8701 - val_loss: 0.0175 - val_accuracy: 0.8659\n",
      "Epoch 30/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0486 - accuracy: 0.8557 - val_loss: 0.1515 - val_accuracy: 0.8216\n",
      "Epoch 31/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.8680 - val_loss: 0.0256 - val_accuracy: 0.8635\n",
      "Epoch 32/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.8722 - val_loss: 0.0223 - val_accuracy: 0.8623\n",
      "Epoch 33/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 5.6870e-04 - accuracy: 0.8734 - val_loss: 0.0224 - val_accuracy: 0.8647\n",
      "Epoch 34/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 1.2366e-04 - accuracy: 0.8734 - val_loss: 0.0228 - val_accuracy: 0.8671\n",
      "Epoch 35/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 3.1528e-04 - accuracy: 0.8734 - val_loss: 0.0186 - val_accuracy: 0.8683\n",
      "Epoch 36/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 5.0169e-04 - accuracy: 0.8734 - val_loss: 0.0203 - val_accuracy: 0.8647\n",
      "Epoch 37/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0013 - accuracy: 0.8728 - val_loss: 0.0151 - val_accuracy: 0.8683\n",
      "Epoch 38/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0018 - accuracy: 0.8725 - val_loss: 0.0260 - val_accuracy: 0.8635\n",
      "Epoch 39/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0024 - accuracy: 0.8725 - val_loss: 0.0256 - val_accuracy: 0.8635\n",
      "Epoch 40/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 0.8725 - val_loss: 0.0327 - val_accuracy: 0.8611\n",
      "Epoch 41/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0021 - accuracy: 0.8725 - val_loss: 0.0337 - val_accuracy: 0.8623\n",
      "Epoch 42/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 0.8728 - val_loss: 0.0618 - val_accuracy: 0.8479\n",
      "Epoch 43/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0146 - accuracy: 0.8692 - val_loss: 0.0437 - val_accuracy: 0.8539\n",
      "Epoch 44/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0161 - accuracy: 0.8683 - val_loss: 0.0712 - val_accuracy: 0.8455\n",
      "Epoch 45/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 0.8719 - val_loss: 0.0449 - val_accuracy: 0.8527\n",
      "Epoch 46/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 0.8725 - val_loss: 0.0157 - val_accuracy: 0.8671\n",
      "Epoch 47/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.7894e-04 - accuracy: 0.8734 - val_loss: 0.0143 - val_accuracy: 0.8683\n",
      "Epoch 48/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 9.8737e-07 - accuracy: 0.8734 - val_loss: 0.0171 - val_accuracy: 0.8671\n",
      "Epoch 49/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 0.8728 - val_loss: 0.0289 - val_accuracy: 0.8611\n",
      "Epoch 50/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 0.8734 - val_loss: 0.0201 - val_accuracy: 0.8647\n",
      "Epoch 51/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 5.1624e-04 - accuracy: 0.8734 - val_loss: 0.0443 - val_accuracy: 0.8599\n",
      "Epoch 52/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 0.8731 - val_loss: 0.0192 - val_accuracy: 0.8647\n",
      "Epoch 53/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 0.8731 - val_loss: 0.1087 - val_accuracy: 0.8311\n",
      "Epoch 54/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.8710 - val_loss: 0.0477 - val_accuracy: 0.8539\n",
      "Epoch 55/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 0.8725 - val_loss: 0.0409 - val_accuracy: 0.8575\n",
      "Epoch 56/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 8.3590e-04 - accuracy: 0.8734 - val_loss: 0.0263 - val_accuracy: 0.8623\n",
      "Epoch 57/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0020 - accuracy: 0.8728 - val_loss: 0.0317 - val_accuracy: 0.8599\n",
      "Epoch 58/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.8719 - val_loss: 0.0309 - val_accuracy: 0.8611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.8671 - val_loss: 0.0324 - val_accuracy: 0.8611\n",
      "Epoch 60/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0205 - accuracy: 0.8659 - val_loss: 0.0332 - val_accuracy: 0.8611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12a8033b550>"
      ]
     },
     "execution_count": 1015,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(chroma, Y, stratify=Y, test_size=0.2)\n",
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=60, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a20c5ae",
   "metadata": {},
   "source": [
    "### Lets try the different features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be586274",
   "metadata": {},
   "source": [
    "## Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "id": "0bc08aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 2ms/step - loss: 0.0332 - accuracy: 0.8611\n",
      "[test loss, test accuracy]: [0.033224258571863174, 0.8610778450965881]\n"
     ]
    }
   ],
   "source": [
    "eval_result = model.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "id": "be4d469d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 44.4344 - accuracy: 0.3731 - val_loss: 2.1137 - val_accuracy: 0.5389\n",
      "Epoch 2/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.2300 - accuracy: 0.4138 - val_loss: 0.9581 - val_accuracy: 0.4731\n",
      "Epoch 3/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.8646 - accuracy: 0.4216 - val_loss: 0.8393 - val_accuracy: 0.4575\n",
      "Epoch 4/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.7878 - accuracy: 0.4210 - val_loss: 0.8358 - val_accuracy: 0.4311\n",
      "Epoch 5/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.7619 - accuracy: 0.4156 - val_loss: 0.7808 - val_accuracy: 0.4802\n",
      "Epoch 6/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.7276 - accuracy: 0.4251 - val_loss: 0.7818 - val_accuracy: 0.4587\n",
      "Epoch 7/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.7064 - accuracy: 0.4246 - val_loss: 0.7716 - val_accuracy: 0.3928\n",
      "Epoch 8/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.7103 - accuracy: 0.4135 - val_loss: 0.7863 - val_accuracy: 0.4611\n",
      "Epoch 9/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.6854 - accuracy: 0.4356 - val_loss: 0.7860 - val_accuracy: 0.4743\n",
      "Epoch 10/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.6830 - accuracy: 0.4398 - val_loss: 0.7723 - val_accuracy: 0.4359\n",
      "Epoch 11/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.6778 - accuracy: 0.4254 - val_loss: 0.7807 - val_accuracy: 0.4766\n",
      "Epoch 12/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.6650 - accuracy: 0.4431 - val_loss: 0.7631 - val_accuracy: 0.4659\n",
      "Epoch 13/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.6649 - accuracy: 0.4467 - val_loss: 0.7809 - val_accuracy: 0.4814\n",
      "Epoch 14/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6528 - accuracy: 0.4509 - val_loss: 0.7991 - val_accuracy: 0.4383\n",
      "Epoch 15/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.6484 - accuracy: 0.4632 - val_loss: 0.7790 - val_accuracy: 0.5126\n",
      "Epoch 16/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.6403 - accuracy: 0.4754 - val_loss: 0.7478 - val_accuracy: 0.5174\n",
      "Epoch 17/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6235 - accuracy: 0.4829 - val_loss: 0.7491 - val_accuracy: 0.5126\n",
      "Epoch 18/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6152 - accuracy: 0.4856 - val_loss: 0.7619 - val_accuracy: 0.5365\n",
      "Epoch 19/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5900 - accuracy: 0.5027 - val_loss: 0.7320 - val_accuracy: 0.4707\n",
      "Epoch 20/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5887 - accuracy: 0.4913 - val_loss: 0.7597 - val_accuracy: 0.5317\n",
      "Epoch 21/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5887 - accuracy: 0.4994 - val_loss: 0.7469 - val_accuracy: 0.5198\n",
      "Epoch 22/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5788 - accuracy: 0.5060 - val_loss: 0.7417 - val_accuracy: 0.5246\n",
      "Epoch 23/60\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.5800 - accuracy: 0.5081 - val_loss: 0.7372 - val_accuracy: 0.5341\n",
      "Epoch 24/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.5625 - accuracy: 0.5323 - val_loss: 0.7542 - val_accuracy: 0.5485\n",
      "Epoch 25/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5570 - accuracy: 0.5150 - val_loss: 0.7401 - val_accuracy: 0.5293\n",
      "Epoch 26/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5516 - accuracy: 0.5102 - val_loss: 0.7652 - val_accuracy: 0.4994\n",
      "Epoch 27/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5507 - accuracy: 0.5257 - val_loss: 0.7395 - val_accuracy: 0.5257\n",
      "Epoch 28/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5513 - accuracy: 0.5257 - val_loss: 0.7557 - val_accuracy: 0.4922\n",
      "Epoch 29/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5456 - accuracy: 0.5231 - val_loss: 0.7282 - val_accuracy: 0.5222\n",
      "Epoch 30/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5448 - accuracy: 0.5249 - val_loss: 0.7416 - val_accuracy: 0.4910\n",
      "Epoch 31/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5410 - accuracy: 0.5225 - val_loss: 0.7464 - val_accuracy: 0.5257\n",
      "Epoch 32/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.5380 - accuracy: 0.5222 - val_loss: 0.7704 - val_accuracy: 0.5521\n",
      "Epoch 33/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5379 - accuracy: 0.5222 - val_loss: 0.7699 - val_accuracy: 0.4850\n",
      "Epoch 34/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5374 - accuracy: 0.5249 - val_loss: 0.7587 - val_accuracy: 0.5497\n",
      "Epoch 35/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.5320 - accuracy: 0.5284 - val_loss: 0.7527 - val_accuracy: 0.5473\n",
      "Epoch 36/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.5352 - accuracy: 0.5344 - val_loss: 0.7695 - val_accuracy: 0.4982\n",
      "Epoch 37/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5311 - accuracy: 0.5293 - val_loss: 0.7744 - val_accuracy: 0.5509\n",
      "Epoch 38/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5337 - accuracy: 0.5338 - val_loss: 0.7789 - val_accuracy: 0.5257\n",
      "Epoch 39/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5313 - accuracy: 0.5251 - val_loss: 0.7595 - val_accuracy: 0.4958\n",
      "Epoch 40/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5264 - accuracy: 0.5272 - val_loss: 0.7656 - val_accuracy: 0.5257\n",
      "Epoch 41/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5263 - accuracy: 0.5362 - val_loss: 0.7728 - val_accuracy: 0.5066\n",
      "Epoch 42/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5218 - accuracy: 0.5335 - val_loss: 0.7666 - val_accuracy: 0.5234\n",
      "Epoch 43/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.5194 - accuracy: 0.5368 - val_loss: 0.7817 - val_accuracy: 0.5257\n",
      "Epoch 44/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5172 - accuracy: 0.5326 - val_loss: 0.7569 - val_accuracy: 0.4898\n",
      "Epoch 45/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5173 - accuracy: 0.5263 - val_loss: 0.7754 - val_accuracy: 0.4731\n",
      "Epoch 46/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.5211 - accuracy: 0.5278 - val_loss: 0.7341 - val_accuracy: 0.4802\n",
      "Epoch 47/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5148 - accuracy: 0.5398 - val_loss: 0.7456 - val_accuracy: 0.4934\n",
      "Epoch 48/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.5123 - accuracy: 0.5419 - val_loss: 0.7607 - val_accuracy: 0.5365\n",
      "Epoch 49/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.5081 - accuracy: 0.5473 - val_loss: 0.7504 - val_accuracy: 0.5114\n",
      "Epoch 50/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5034 - accuracy: 0.5539 - val_loss: 0.7604 - val_accuracy: 0.5341\n",
      "Epoch 51/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.4930 - accuracy: 0.5530 - val_loss: 0.7643 - val_accuracy: 0.5006\n",
      "Epoch 52/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.4979 - accuracy: 0.5512 - val_loss: 0.7449 - val_accuracy: 0.5305\n",
      "Epoch 53/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.4939 - accuracy: 0.5557 - val_loss: 0.7508 - val_accuracy: 0.5281\n",
      "Epoch 54/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.4901 - accuracy: 0.5572 - val_loss: 0.7800 - val_accuracy: 0.5054\n",
      "Epoch 55/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.4853 - accuracy: 0.5605 - val_loss: 0.7550 - val_accuracy: 0.5090\n",
      "Epoch 56/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4862 - accuracy: 0.5491 - val_loss: 0.7990 - val_accuracy: 0.4886\n",
      "Epoch 57/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4744 - accuracy: 0.5509 - val_loss: 0.8094 - val_accuracy: 0.5593\n",
      "Epoch 58/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4729 - accuracy: 0.5509 - val_loss: 0.8096 - val_accuracy: 0.5605\n",
      "Epoch 59/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4760 - accuracy: 0.5491 - val_loss: 0.7672 - val_accuracy: 0.5030\n",
      "Epoch 60/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4676 - accuracy: 0.5698 - val_loss: 0.8099 - val_accuracy: 0.4778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12aa953dcc0>"
      ]
     },
     "execution_count": 1017,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(mfcc, Y, stratify=Y, test_size=0.2)\n",
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=60, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca55951a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58bc8194",
   "metadata": {},
   "source": [
    "## MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "id": "2ccd0297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 2ms/step - loss: 0.8098 - accuracy: 0.4778\n",
      "[test loss, test accuracy]: [0.8098351955413818, 0.4778442978858948]\n"
     ]
    }
   ],
   "source": [
    "eval_result = model.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "id": "725a5198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_35 (Flatten)        (None, 469)               0         \n",
      "                                                                 \n",
      " dense_175 (Dense)           (None, 96)                45120     \n",
      "                                                                 \n",
      " dense_176 (Dense)           (None, 64)                6208      \n",
      "                                                                 \n",
      " dense_177 (Dense)           (None, 192)               12480     \n",
      "                                                                 \n",
      " dense_178 (Dense)           (None, 192)               37056     \n",
      "                                                                 \n",
      " dense_179 (Dense)           (None, 64)                12352     \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_180 (Dense)           (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 113,671\n",
      "Trainable params: 113,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/60\n",
      "53/53 [==============================] - 1s 9ms/step - loss: 469.7886 - accuracy: 0.1698 - val_loss: 75.7602 - val_accuracy: 0.2790\n",
      "Epoch 2/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 65.3317 - accuracy: 0.2204 - val_loss: 24.8574 - val_accuracy: 0.2874\n",
      "Epoch 3/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 17.4663 - accuracy: 0.2177 - val_loss: 8.1545 - val_accuracy: 0.2683\n",
      "Epoch 4/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 5.7144 - accuracy: 0.2716 - val_loss: 3.9927 - val_accuracy: 0.2874\n",
      "Epoch 5/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 3.1782 - accuracy: 0.2383 - val_loss: 2.7172 - val_accuracy: 0.2695\n",
      "Epoch 6/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 2.1524 - accuracy: 0.2120 - val_loss: 2.1060 - val_accuracy: 0.1341\n",
      "Epoch 7/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.7063 - accuracy: 0.2204 - val_loss: 1.8447 - val_accuracy: 0.1293\n",
      "Epoch 8/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.4357 - accuracy: 0.2039 - val_loss: 1.6981 - val_accuracy: 0.2599\n",
      "Epoch 9/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.3164 - accuracy: 0.2272 - val_loss: 1.6622 - val_accuracy: 0.1365\n",
      "Epoch 10/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.2208 - accuracy: 0.2078 - val_loss: 1.6024 - val_accuracy: 0.2575\n",
      "Epoch 11/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.1331 - accuracy: 0.2045 - val_loss: 1.5749 - val_accuracy: 0.2563\n",
      "Epoch 12/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0935 - accuracy: 0.2063 - val_loss: 1.5730 - val_accuracy: 0.1521\n",
      "Epoch 13/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0570 - accuracy: 0.2075 - val_loss: 1.5608 - val_accuracy: 0.2515\n",
      "Epoch 14/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0364 - accuracy: 0.2180 - val_loss: 1.5294 - val_accuracy: 0.1365\n",
      "Epoch 15/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0533 - accuracy: 0.2006 - val_loss: 1.5174 - val_accuracy: 0.2527\n",
      "Epoch 16/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0218 - accuracy: 0.2012 - val_loss: 1.5005 - val_accuracy: 0.1234\n",
      "Epoch 17/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0065 - accuracy: 0.2141 - val_loss: 1.5149 - val_accuracy: 0.1281\n",
      "Epoch 18/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0037 - accuracy: 0.2084 - val_loss: 1.5143 - val_accuracy: 0.1353\n",
      "Epoch 19/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0057 - accuracy: 0.2033 - val_loss: 1.5158 - val_accuracy: 0.1281\n",
      "Epoch 20/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9887 - accuracy: 0.2078 - val_loss: 1.5108 - val_accuracy: 0.2515\n",
      "Epoch 21/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.9974 - accuracy: 0.2123 - val_loss: 1.5011 - val_accuracy: 0.2515\n",
      "Epoch 22/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0114 - accuracy: 0.2153 - val_loss: 1.4878 - val_accuracy: 0.1497\n",
      "Epoch 23/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9833 - accuracy: 0.2162 - val_loss: 1.4836 - val_accuracy: 0.1497\n",
      "Epoch 24/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.9851 - accuracy: 0.2066 - val_loss: 1.4892 - val_accuracy: 0.2551\n",
      "Epoch 25/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.9854 - accuracy: 0.2225 - val_loss: 1.4859 - val_accuracy: 0.1257\n",
      "Epoch 26/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0036 - accuracy: 0.1931 - val_loss: 1.4873 - val_accuracy: 0.1281\n",
      "Epoch 27/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0033 - accuracy: 0.2066 - val_loss: 1.4769 - val_accuracy: 0.1317\n",
      "Epoch 28/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 1.0023 - accuracy: 0.2150 - val_loss: 1.4768 - val_accuracy: 0.1293\n",
      "Epoch 29/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0201 - accuracy: 0.2012 - val_loss: 1.4828 - val_accuracy: 0.1293\n",
      "Epoch 30/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0037 - accuracy: 0.2009 - val_loss: 1.4715 - val_accuracy: 0.2527\n",
      "Epoch 31/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9882 - accuracy: 0.2051 - val_loss: 1.4634 - val_accuracy: 0.2539\n",
      "Epoch 32/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9954 - accuracy: 0.2132 - val_loss: 1.4554 - val_accuracy: 0.2563\n",
      "Epoch 33/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9826 - accuracy: 0.2114 - val_loss: 1.4557 - val_accuracy: 0.1317\n",
      "Epoch 34/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9841 - accuracy: 0.2048 - val_loss: 1.4498 - val_accuracy: 0.1234\n",
      "Epoch 35/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9954 - accuracy: 0.2003 - val_loss: 1.4477 - val_accuracy: 0.2527\n",
      "Epoch 36/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9824 - accuracy: 0.1994 - val_loss: 1.4535 - val_accuracy: 0.2527\n",
      "Epoch 37/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.9948 - accuracy: 0.1994 - val_loss: 1.4439 - val_accuracy: 0.2575\n",
      "Epoch 38/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9860 - accuracy: 0.2096 - val_loss: 1.4423 - val_accuracy: 0.2575\n",
      "Epoch 39/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9863 - accuracy: 0.2063 - val_loss: 1.4473 - val_accuracy: 0.2575\n",
      "Epoch 40/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9823 - accuracy: 0.2126 - val_loss: 1.4463 - val_accuracy: 0.1257\n",
      "Epoch 41/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9886 - accuracy: 0.2066 - val_loss: 1.4440 - val_accuracy: 0.1509\n",
      "Epoch 42/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9821 - accuracy: 0.2102 - val_loss: 1.4435 - val_accuracy: 0.2563\n",
      "Epoch 43/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9838 - accuracy: 0.2105 - val_loss: 1.4412 - val_accuracy: 0.1341\n",
      "Epoch 44/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9825 - accuracy: 0.1997 - val_loss: 1.4360 - val_accuracy: 0.1509\n",
      "Epoch 45/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9800 - accuracy: 0.2108 - val_loss: 1.4346 - val_accuracy: 0.1341\n",
      "Epoch 46/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9835 - accuracy: 0.2018 - val_loss: 1.4360 - val_accuracy: 0.2551\n",
      "Epoch 47/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9896 - accuracy: 0.2150 - val_loss: 1.4279 - val_accuracy: 0.1257\n",
      "Epoch 48/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9951 - accuracy: 0.2072 - val_loss: 1.4350 - val_accuracy: 0.2551\n",
      "Epoch 49/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9791 - accuracy: 0.2132 - val_loss: 1.4408 - val_accuracy: 0.1257\n",
      "Epoch 50/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9870 - accuracy: 0.2033 - val_loss: 1.4359 - val_accuracy: 0.1257\n",
      "Epoch 51/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9814 - accuracy: 0.1976 - val_loss: 1.4369 - val_accuracy: 0.2551\n",
      "Epoch 52/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9803 - accuracy: 0.1964 - val_loss: 1.4483 - val_accuracy: 0.2563\n",
      "Epoch 53/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9831 - accuracy: 0.2024 - val_loss: 1.4351 - val_accuracy: 0.2551\n",
      "Epoch 54/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9850 - accuracy: 0.2180 - val_loss: 1.4192 - val_accuracy: 0.2563\n",
      "Epoch 55/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.9796 - accuracy: 0.2099 - val_loss: 1.4323 - val_accuracy: 0.2551\n",
      "Epoch 56/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.9804 - accuracy: 0.2129 - val_loss: 1.4335 - val_accuracy: 0.1377\n",
      "Epoch 57/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.9843 - accuracy: 0.2180 - val_loss: 1.4393 - val_accuracy: 0.2551\n",
      "Epoch 58/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9887 - accuracy: 0.2075 - val_loss: 1.4626 - val_accuracy: 0.2551\n",
      "Epoch 59/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0045 - accuracy: 0.2060 - val_loss: 1.4467 - val_accuracy: 0.2551\n",
      "Epoch 60/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9845 - accuracy: 0.2150 - val_loss: 1.4342 - val_accuracy: 0.2539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12a80372a40>"
      ]
     },
     "execution_count": 1019,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(1, 469, 1)))\n",
    "\n",
    "model.add(keras.layers.Dense(units=best_hps.get(\"units\"), activation='relu'))\n",
    "model.add(keras.layers.Dense(units=best_hps.get(\"units1\"), activation='relu'))\n",
    "model.add(keras.layers.Dense(units=best_hps.get(\"units2\"), activation='relu'))\n",
    "model.add(keras.layers.Dense(units=best_hps.get(\"units3\"), activation='relu'))\n",
    "model.add(keras.layers.Dense(units=best_hps.get(\"units4\"), activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(rate=best_hps.get(\"drop\")),)\n",
    "    \n",
    "model.add(keras.layers.Dense(7))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=best_hps.get(\"learning_rate\")),\n",
    "                loss=keras.losses.CategoricalHinge(),\n",
    "                metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "trainX, testX, trainY, testY = train_test_split(spectral, Y, stratify=Y, test_size=0.2)\n",
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=60, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ad766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa344572",
   "metadata": {},
   "source": [
    "## Spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "id": "4e7ff21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 4ms/step - loss: 1.4342 - accuracy: 0.2539\n",
      "[test loss, test accuracy]: [1.4342151880264282, 0.2538922131061554]\n"
     ]
    }
   ],
   "source": [
    "eval_result = model.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "id": "aee9e1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "53/53 [==============================] - 1s 11ms/step - loss: 3.6185 - accuracy: 0.3431 - val_loss: 1.0803 - val_accuracy: 0.5281\n",
      "Epoch 2/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 1.2643 - accuracy: 0.5090 - val_loss: 0.7372 - val_accuracy: 0.6431\n",
      "Epoch 3/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.8781 - accuracy: 0.5958 - val_loss: 0.6550 - val_accuracy: 0.6683\n",
      "Epoch 4/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.6951 - accuracy: 0.6362 - val_loss: 0.5898 - val_accuracy: 0.6754\n",
      "Epoch 5/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.6048 - accuracy: 0.6620 - val_loss: 0.5807 - val_accuracy: 0.6802\n",
      "Epoch 6/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5177 - accuracy: 0.6895 - val_loss: 0.5525 - val_accuracy: 0.6946\n",
      "Epoch 7/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.4707 - accuracy: 0.6994 - val_loss: 0.5077 - val_accuracy: 0.6958\n",
      "Epoch 8/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4409 - accuracy: 0.7210 - val_loss: 0.4875 - val_accuracy: 0.6886\n",
      "Epoch 9/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3806 - accuracy: 0.7344 - val_loss: 0.4900 - val_accuracy: 0.6922\n",
      "Epoch 10/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3678 - accuracy: 0.7362 - val_loss: 0.5216 - val_accuracy: 0.6982\n",
      "Epoch 11/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3342 - accuracy: 0.7548 - val_loss: 0.4905 - val_accuracy: 0.7174\n",
      "Epoch 12/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3385 - accuracy: 0.7530 - val_loss: 0.4821 - val_accuracy: 0.7066\n",
      "Epoch 13/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3002 - accuracy: 0.7644 - val_loss: 0.4799 - val_accuracy: 0.7030\n",
      "Epoch 14/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2774 - accuracy: 0.7686 - val_loss: 0.4763 - val_accuracy: 0.7126\n",
      "Epoch 15/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2773 - accuracy: 0.7725 - val_loss: 0.5055 - val_accuracy: 0.6934\n",
      "Epoch 16/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2571 - accuracy: 0.7751 - val_loss: 0.4898 - val_accuracy: 0.6934\n",
      "Epoch 17/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2272 - accuracy: 0.7895 - val_loss: 0.4968 - val_accuracy: 0.7126\n",
      "Epoch 18/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2341 - accuracy: 0.7898 - val_loss: 0.4726 - val_accuracy: 0.7102\n",
      "Epoch 19/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2366 - accuracy: 0.7847 - val_loss: 0.4879 - val_accuracy: 0.6994\n",
      "Epoch 20/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2277 - accuracy: 0.7865 - val_loss: 0.4953 - val_accuracy: 0.6934\n",
      "Epoch 21/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1994 - accuracy: 0.7982 - val_loss: 0.4877 - val_accuracy: 0.7030\n",
      "Epoch 22/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1811 - accuracy: 0.8033 - val_loss: 0.4724 - val_accuracy: 0.6910\n",
      "Epoch 23/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1954 - accuracy: 0.7949 - val_loss: 0.5206 - val_accuracy: 0.6982\n",
      "Epoch 24/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1851 - accuracy: 0.8039 - val_loss: 0.5475 - val_accuracy: 0.7054\n",
      "Epoch 25/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1798 - accuracy: 0.8063 - val_loss: 0.5526 - val_accuracy: 0.7090\n",
      "Epoch 26/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1764 - accuracy: 0.8123 - val_loss: 0.5563 - val_accuracy: 0.7054\n",
      "Epoch 27/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1753 - accuracy: 0.8078 - val_loss: 0.5190 - val_accuracy: 0.7078\n",
      "Epoch 28/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1741 - accuracy: 0.8120 - val_loss: 0.4951 - val_accuracy: 0.7150\n",
      "Epoch 29/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1601 - accuracy: 0.8222 - val_loss: 0.5304 - val_accuracy: 0.7102\n",
      "Epoch 30/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1482 - accuracy: 0.8228 - val_loss: 0.5417 - val_accuracy: 0.6970\n",
      "Epoch 31/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1624 - accuracy: 0.8153 - val_loss: 0.6525 - val_accuracy: 0.6922\n",
      "Epoch 32/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1630 - accuracy: 0.8141 - val_loss: 0.5411 - val_accuracy: 0.6946\n",
      "Epoch 33/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1390 - accuracy: 0.8231 - val_loss: 0.5293 - val_accuracy: 0.7174\n",
      "Epoch 34/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1371 - accuracy: 0.8263 - val_loss: 0.5338 - val_accuracy: 0.7066\n",
      "Epoch 35/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1244 - accuracy: 0.8308 - val_loss: 0.5255 - val_accuracy: 0.7030\n",
      "Epoch 36/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1208 - accuracy: 0.8344 - val_loss: 0.5574 - val_accuracy: 0.7042\n",
      "Epoch 37/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1227 - accuracy: 0.8305 - val_loss: 0.5397 - val_accuracy: 0.7174\n",
      "Epoch 38/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1171 - accuracy: 0.8317 - val_loss: 0.5984 - val_accuracy: 0.7066\n",
      "Epoch 39/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1168 - accuracy: 0.8269 - val_loss: 0.5095 - val_accuracy: 0.7138\n",
      "Epoch 40/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0991 - accuracy: 0.8386 - val_loss: 0.6155 - val_accuracy: 0.6994\n",
      "Epoch 41/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1006 - accuracy: 0.8350 - val_loss: 0.6144 - val_accuracy: 0.6982\n",
      "Epoch 42/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1005 - accuracy: 0.8338 - val_loss: 0.6598 - val_accuracy: 0.6934\n",
      "Epoch 43/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0934 - accuracy: 0.8398 - val_loss: 0.6329 - val_accuracy: 0.6970\n",
      "Epoch 44/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0935 - accuracy: 0.8380 - val_loss: 0.6094 - val_accuracy: 0.7066\n",
      "Epoch 45/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1233 - accuracy: 0.8305 - val_loss: 0.5780 - val_accuracy: 0.7138\n",
      "Epoch 46/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1046 - accuracy: 0.8383 - val_loss: 0.6000 - val_accuracy: 0.7102\n",
      "Epoch 47/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0748 - accuracy: 0.8467 - val_loss: 0.5804 - val_accuracy: 0.7138\n",
      "Epoch 48/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0691 - accuracy: 0.8482 - val_loss: 0.5906 - val_accuracy: 0.6958\n",
      "Epoch 49/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0764 - accuracy: 0.8482 - val_loss: 0.6378 - val_accuracy: 0.6982\n",
      "Epoch 50/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1054 - accuracy: 0.8359 - val_loss: 0.6276 - val_accuracy: 0.7078\n",
      "Epoch 51/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0970 - accuracy: 0.8344 - val_loss: 0.6150 - val_accuracy: 0.7066\n",
      "Epoch 52/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0950 - accuracy: 0.8368 - val_loss: 0.6333 - val_accuracy: 0.7078\n",
      "Epoch 53/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0803 - accuracy: 0.8458 - val_loss: 0.6228 - val_accuracy: 0.6994\n",
      "Epoch 54/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0863 - accuracy: 0.8404 - val_loss: 0.6251 - val_accuracy: 0.7054\n",
      "Epoch 55/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0680 - accuracy: 0.8494 - val_loss: 0.7126 - val_accuracy: 0.6970\n",
      "Epoch 56/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0728 - accuracy: 0.8467 - val_loss: 0.7272 - val_accuracy: 0.7030\n",
      "Epoch 57/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0888 - accuracy: 0.8413 - val_loss: 0.6299 - val_accuracy: 0.7042\n",
      "Epoch 58/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0696 - accuracy: 0.8485 - val_loss: 0.7026 - val_accuracy: 0.6946\n",
      "Epoch 59/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0644 - accuracy: 0.8515 - val_loss: 0.6532 - val_accuracy: 0.7054\n",
      "Epoch 60/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0386 - accuracy: 0.8626 - val_loss: 0.6644 - val_accuracy: 0.7150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12abbd07a00>"
      ]
     },
     "execution_count": 1021,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(2, 10, 469, 1)))\n",
    "\n",
    "model.add(keras.layers.Dense(units=fcbest_hps.get(\"units\"), activation='relu'))\n",
    "model.add(keras.layers.Dense(units=fcbest_hps.get(\"units1\"), activation='relu'))\n",
    "model.add(keras.layers.Dense(units=fcbest_hps.get(\"units2\"), activation='relu'))\n",
    "model.add(keras.layers.Dense(units=fcbest_hps.get(\"units3\"), activation='relu'))\n",
    "model.add(keras.layers.Dense(units=fcbest_hps.get(\"units4\"), activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(rate=fcbest_hps.get(\"drop\")),)\n",
    "    \n",
    "model.add(keras.layers.Dense(7))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=fcbest_hps.get(\"learning_rate\")),\n",
    "                loss=keras.losses.CategoricalHinge(),\n",
    "                metrics=['accuracy'])\n",
    "chromamfcc = np.concatenate((chroma,mfcc),axis=1)\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(chromamfcc, Y, stratify=Y, test_size=0.2)\n",
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=60, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f2f7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df0a81fe",
   "metadata": {},
   "source": [
    "## Chroma + MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "id": "26706cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 2ms/step - loss: 0.6644 - accuracy: 0.7150\n",
      "[test loss, test accuracy]: [0.6643737554550171, 0.714970052242279]\n"
     ]
    }
   ],
   "source": [
    "eval_result = model.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "id": "fd536a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "53/53 [==============================] - 1s 9ms/step - loss: 1.0137 - accuracy: 0.2320 - val_loss: 0.9991 - val_accuracy: 0.3257\n",
      "Epoch 2/60\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.9972 - accuracy: 0.3674 - val_loss: 0.9933 - val_accuracy: 0.4251\n",
      "Epoch 3/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9909 - accuracy: 0.4344 - val_loss: 0.9861 - val_accuracy: 0.4527\n",
      "Epoch 4/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9808 - accuracy: 0.4467 - val_loss: 0.9745 - val_accuracy: 0.4611\n",
      "Epoch 5/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9646 - accuracy: 0.4665 - val_loss: 0.9598 - val_accuracy: 0.4299\n",
      "Epoch 6/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9385 - accuracy: 0.4614 - val_loss: 0.9232 - val_accuracy: 0.4527\n",
      "Epoch 7/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.8817 - accuracy: 0.4641 - val_loss: 0.8473 - val_accuracy: 0.4467\n",
      "Epoch 8/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.8150 - accuracy: 0.4734 - val_loss: 0.8559 - val_accuracy: 0.4287\n",
      "Epoch 9/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.7815 - accuracy: 0.4808 - val_loss: 0.7695 - val_accuracy: 0.4743\n",
      "Epoch 10/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.7224 - accuracy: 0.5102 - val_loss: 0.7496 - val_accuracy: 0.4754\n",
      "Epoch 11/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.6995 - accuracy: 0.5177 - val_loss: 0.7246 - val_accuracy: 0.4838\n",
      "Epoch 12/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.6718 - accuracy: 0.5257 - val_loss: 0.6916 - val_accuracy: 0.4982\n",
      "Epoch 13/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.6448 - accuracy: 0.5359 - val_loss: 0.6802 - val_accuracy: 0.5150\n",
      "Epoch 14/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.6306 - accuracy: 0.5488 - val_loss: 0.6515 - val_accuracy: 0.5293\n",
      "Epoch 15/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.6108 - accuracy: 0.5605 - val_loss: 0.6384 - val_accuracy: 0.5461\n",
      "Epoch 16/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.5902 - accuracy: 0.5766 - val_loss: 0.6590 - val_accuracy: 0.5138\n",
      "Epoch 17/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5807 - accuracy: 0.5820 - val_loss: 0.6332 - val_accuracy: 0.5784\n",
      "Epoch 18/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5617 - accuracy: 0.5976 - val_loss: 0.6585 - val_accuracy: 0.5246\n",
      "Epoch 19/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.5486 - accuracy: 0.5994 - val_loss: 0.6021 - val_accuracy: 0.5916\n",
      "Epoch 20/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.5335 - accuracy: 0.6171 - val_loss: 0.5938 - val_accuracy: 0.5916\n",
      "Epoch 21/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.5221 - accuracy: 0.6251 - val_loss: 0.5909 - val_accuracy: 0.6000\n",
      "Epoch 22/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.5127 - accuracy: 0.6341 - val_loss: 0.5780 - val_accuracy: 0.6036\n",
      "Epoch 23/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.5008 - accuracy: 0.6362 - val_loss: 0.5785 - val_accuracy: 0.6216\n",
      "Epoch 24/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4920 - accuracy: 0.6434 - val_loss: 0.5854 - val_accuracy: 0.5964\n",
      "Epoch 25/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.4815 - accuracy: 0.6506 - val_loss: 0.5627 - val_accuracy: 0.6263\n",
      "Epoch 26/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4714 - accuracy: 0.6617 - val_loss: 0.5724 - val_accuracy: 0.6012\n",
      "Epoch 27/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4613 - accuracy: 0.6629 - val_loss: 0.5548 - val_accuracy: 0.6299\n",
      "Epoch 28/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4579 - accuracy: 0.6707 - val_loss: 0.5436 - val_accuracy: 0.6335\n",
      "Epoch 29/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4493 - accuracy: 0.6704 - val_loss: 0.5589 - val_accuracy: 0.6251\n",
      "Epoch 30/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4486 - accuracy: 0.6728 - val_loss: 0.5628 - val_accuracy: 0.6036\n",
      "Epoch 31/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4345 - accuracy: 0.6784 - val_loss: 0.5455 - val_accuracy: 0.6144\n",
      "Epoch 32/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.4284 - accuracy: 0.6850 - val_loss: 0.5392 - val_accuracy: 0.6335\n",
      "Epoch 33/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.4298 - accuracy: 0.6802 - val_loss: 0.5408 - val_accuracy: 0.6311\n",
      "Epoch 34/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.4116 - accuracy: 0.6949 - val_loss: 0.5356 - val_accuracy: 0.6371\n",
      "Epoch 35/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.4059 - accuracy: 0.6967 - val_loss: 0.5312 - val_accuracy: 0.6407\n",
      "Epoch 36/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3962 - accuracy: 0.7021 - val_loss: 0.5304 - val_accuracy: 0.6407\n",
      "Epoch 37/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3926 - accuracy: 0.7054 - val_loss: 0.5310 - val_accuracy: 0.6395\n",
      "Epoch 38/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3867 - accuracy: 0.7042 - val_loss: 0.5279 - val_accuracy: 0.6335\n",
      "Epoch 39/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3828 - accuracy: 0.7072 - val_loss: 0.5249 - val_accuracy: 0.6383\n",
      "Epoch 40/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3771 - accuracy: 0.7123 - val_loss: 0.5177 - val_accuracy: 0.6431\n",
      "Epoch 41/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3682 - accuracy: 0.7114 - val_loss: 0.5266 - val_accuracy: 0.6455\n",
      "Epoch 42/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3683 - accuracy: 0.7138 - val_loss: 0.5362 - val_accuracy: 0.6359\n",
      "Epoch 43/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3536 - accuracy: 0.7198 - val_loss: 0.5386 - val_accuracy: 0.6371\n",
      "Epoch 44/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3551 - accuracy: 0.7174 - val_loss: 0.5290 - val_accuracy: 0.6359\n",
      "Epoch 45/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3554 - accuracy: 0.7207 - val_loss: 0.5123 - val_accuracy: 0.6455\n",
      "Epoch 46/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3386 - accuracy: 0.7198 - val_loss: 0.5093 - val_accuracy: 0.6455\n",
      "Epoch 47/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3325 - accuracy: 0.7275 - val_loss: 0.5110 - val_accuracy: 0.6515\n",
      "Epoch 48/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3268 - accuracy: 0.7243 - val_loss: 0.5265 - val_accuracy: 0.6407\n",
      "Epoch 49/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3306 - accuracy: 0.7263 - val_loss: 0.5352 - val_accuracy: 0.6323\n",
      "Epoch 50/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.7350 - val_loss: 0.5125 - val_accuracy: 0.6443\n",
      "Epoch 51/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.7338 - val_loss: 0.5057 - val_accuracy: 0.6491\n",
      "Epoch 52/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3000 - accuracy: 0.7371 - val_loss: 0.5189 - val_accuracy: 0.6371\n",
      "Epoch 53/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.3004 - accuracy: 0.7359 - val_loss: 0.5062 - val_accuracy: 0.6467\n",
      "Epoch 54/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2929 - accuracy: 0.7413 - val_loss: 0.5053 - val_accuracy: 0.6431\n",
      "Epoch 55/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2897 - accuracy: 0.7395 - val_loss: 0.5304 - val_accuracy: 0.6455\n",
      "Epoch 56/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2969 - accuracy: 0.7368 - val_loss: 0.4922 - val_accuracy: 0.6467\n",
      "Epoch 57/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2762 - accuracy: 0.7467 - val_loss: 0.5029 - val_accuracy: 0.6395\n",
      "Epoch 58/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2792 - accuracy: 0.7446 - val_loss: 0.4909 - val_accuracy: 0.6455\n",
      "Epoch 59/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2706 - accuracy: 0.7530 - val_loss: 0.5020 - val_accuracy: 0.6467\n",
      "Epoch 60/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2633 - accuracy: 0.7500 - val_loss: 0.4983 - val_accuracy: 0.6515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12abab23100>"
      ]
     },
     "execution_count": 1023,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(zcross, Y, stratify=Y, test_size=0.2)\n",
    "smodel.fit(trainX, trainY, validation_data=(testX, testY), epochs=60, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c65b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14b66825",
   "metadata": {},
   "source": [
    "## Zcross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "id": "25a4721d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 2ms/step - loss: 0.4983 - accuracy: 0.6515\n",
      "[test loss, test accuracy]: [0.49834349751472473, 0.6514970064163208]\n"
     ]
    }
   ],
   "source": [
    "eval_result = smodel.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d41e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "id": "01c1cbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "53/53 [==============================] - 1s 9ms/step - loss: 168.7832 - accuracy: 0.1877 - val_loss: 32.2623 - val_accuracy: 0.2515\n",
      "Epoch 2/60\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 37.2635 - accuracy: 0.2488 - val_loss: 19.1136 - val_accuracy: 0.2814\n",
      "Epoch 3/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 19.3978 - accuracy: 0.2907 - val_loss: 11.1170 - val_accuracy: 0.3353\n",
      "Epoch 4/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 10.6378 - accuracy: 0.2925 - val_loss: 6.9922 - val_accuracy: 0.3257\n",
      "Epoch 5/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 4.7520 - accuracy: 0.3284 - val_loss: 3.5512 - val_accuracy: 0.2778\n",
      "Epoch 6/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 2.2093 - accuracy: 0.2889 - val_loss: 2.2478 - val_accuracy: 0.2838\n",
      "Epoch 7/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.5878 - accuracy: 0.2467 - val_loss: 1.8547 - val_accuracy: 0.2778\n",
      "Epoch 8/60\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 1.3358 - accuracy: 0.2141 - val_loss: 1.6060 - val_accuracy: 0.2671\n",
      "Epoch 9/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.1861 - accuracy: 0.2045 - val_loss: 1.5145 - val_accuracy: 0.1377\n",
      "Epoch 10/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.1121 - accuracy: 0.2003 - val_loss: 1.4878 - val_accuracy: 0.1365\n",
      "Epoch 11/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.1029 - accuracy: 0.1994 - val_loss: 1.4488 - val_accuracy: 0.1545\n",
      "Epoch 12/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0687 - accuracy: 0.2033 - val_loss: 1.4058 - val_accuracy: 0.2611\n",
      "Epoch 13/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0527 - accuracy: 0.2036 - val_loss: 1.3942 - val_accuracy: 0.2635\n",
      "Epoch 14/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0390 - accuracy: 0.2027 - val_loss: 1.3518 - val_accuracy: 0.2611\n",
      "Epoch 15/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0314 - accuracy: 0.2254 - val_loss: 1.3415 - val_accuracy: 0.1545\n",
      "Epoch 16/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0119 - accuracy: 0.2102 - val_loss: 1.3413 - val_accuracy: 0.2611\n",
      "Epoch 17/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0031 - accuracy: 0.2057 - val_loss: 1.3289 - val_accuracy: 0.1257\n",
      "Epoch 18/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0049 - accuracy: 0.2105 - val_loss: 1.3193 - val_accuracy: 0.1389\n",
      "Epoch 19/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 1.0021 - accuracy: 0.2135 - val_loss: 1.2874 - val_accuracy: 0.1533\n",
      "Epoch 20/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9920 - accuracy: 0.2069 - val_loss: 1.2912 - val_accuracy: 0.2623\n",
      "Epoch 21/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9966 - accuracy: 0.2126 - val_loss: 1.2740 - val_accuracy: 0.2647\n",
      "Epoch 22/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9940 - accuracy: 0.2063 - val_loss: 1.2647 - val_accuracy: 0.1305\n",
      "Epoch 23/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9834 - accuracy: 0.1985 - val_loss: 1.2617 - val_accuracy: 0.2623\n",
      "Epoch 24/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9879 - accuracy: 0.2072 - val_loss: 1.2427 - val_accuracy: 0.2647\n",
      "Epoch 25/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9872 - accuracy: 0.2093 - val_loss: 1.2375 - val_accuracy: 0.1317\n",
      "Epoch 26/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9839 - accuracy: 0.2087 - val_loss: 1.2187 - val_accuracy: 0.1569\n",
      "Epoch 27/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9897 - accuracy: 0.2033 - val_loss: 1.2041 - val_accuracy: 0.1401\n",
      "Epoch 28/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9836 - accuracy: 0.2060 - val_loss: 1.2028 - val_accuracy: 0.2635\n",
      "Epoch 29/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9841 - accuracy: 0.2153 - val_loss: 1.2066 - val_accuracy: 0.2623\n",
      "Epoch 30/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9833 - accuracy: 0.2117 - val_loss: 1.2135 - val_accuracy: 0.2635\n",
      "Epoch 31/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9806 - accuracy: 0.2072 - val_loss: 1.2069 - val_accuracy: 0.1341\n",
      "Epoch 32/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9831 - accuracy: 0.2111 - val_loss: 1.2039 - val_accuracy: 0.2659\n",
      "Epoch 33/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9830 - accuracy: 0.1979 - val_loss: 1.2284 - val_accuracy: 0.2623\n",
      "Epoch 34/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9782 - accuracy: 0.2090 - val_loss: 1.2231 - val_accuracy: 0.1329\n",
      "Epoch 35/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.9803 - accuracy: 0.2168 - val_loss: 1.2321 - val_accuracy: 0.2623\n",
      "Epoch 36/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.9858 - accuracy: 0.2051 - val_loss: 1.2194 - val_accuracy: 0.1305\n",
      "Epoch 37/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.9822 - accuracy: 0.2039 - val_loss: 1.2218 - val_accuracy: 0.2623\n",
      "Epoch 38/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.9814 - accuracy: 0.2105 - val_loss: 1.2158 - val_accuracy: 0.2623\n",
      "Epoch 39/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.9815 - accuracy: 0.2078 - val_loss: 1.2095 - val_accuracy: 0.1329\n",
      "Epoch 40/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.9809 - accuracy: 0.2036 - val_loss: 1.2191 - val_accuracy: 0.2623\n",
      "Epoch 41/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.9776 - accuracy: 0.2069 - val_loss: 1.2132 - val_accuracy: 0.1329\n",
      "Epoch 42/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.9827 - accuracy: 0.1985 - val_loss: 1.2296 - val_accuracy: 0.1413\n",
      "Epoch 43/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9858 - accuracy: 0.2048 - val_loss: 1.2113 - val_accuracy: 0.2659\n",
      "Epoch 44/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9779 - accuracy: 0.2099 - val_loss: 1.1964 - val_accuracy: 0.2647\n",
      "Epoch 45/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9825 - accuracy: 0.2174 - val_loss: 1.1920 - val_accuracy: 0.1401\n",
      "Epoch 46/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9811 - accuracy: 0.1979 - val_loss: 1.1993 - val_accuracy: 0.2647\n",
      "Epoch 47/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9779 - accuracy: 0.2144 - val_loss: 1.1960 - val_accuracy: 0.1341\n",
      "Epoch 48/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9786 - accuracy: 0.2012 - val_loss: 1.1941 - val_accuracy: 0.2635\n",
      "Epoch 49/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9790 - accuracy: 0.2204 - val_loss: 1.1816 - val_accuracy: 0.1377\n",
      "Epoch 50/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9819 - accuracy: 0.2042 - val_loss: 1.1753 - val_accuracy: 0.1305\n",
      "Epoch 51/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9787 - accuracy: 0.2141 - val_loss: 1.1731 - val_accuracy: 0.2599\n",
      "Epoch 52/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9792 - accuracy: 0.2060 - val_loss: 1.1719 - val_accuracy: 0.1257\n",
      "Epoch 53/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9800 - accuracy: 0.2027 - val_loss: 1.1784 - val_accuracy: 0.2587\n",
      "Epoch 54/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9775 - accuracy: 0.2036 - val_loss: 1.1774 - val_accuracy: 0.1341\n",
      "Epoch 55/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9767 - accuracy: 0.1997 - val_loss: 1.1740 - val_accuracy: 0.2575\n",
      "Epoch 56/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9769 - accuracy: 0.1985 - val_loss: 1.1681 - val_accuracy: 0.2599\n",
      "Epoch 57/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9803 - accuracy: 0.2021 - val_loss: 1.1703 - val_accuracy: 0.1257\n",
      "Epoch 58/60\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.9783 - accuracy: 0.2150 - val_loss: 1.1838 - val_accuracy: 0.1293\n",
      "Epoch 59/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.9788 - accuracy: 0.2066 - val_loss: 1.1744 - val_accuracy: 0.2587\n",
      "Epoch 60/60\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.9790 - accuracy: 0.2066 - val_loss: 1.1667 - val_accuracy: 0.2587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12abac7f7f0>"
      ]
     },
     "execution_count": 1025,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(2, 469, 1)))\n",
    "\n",
    "model.add(keras.layers.Dense(units=fcbest_hps.get(\"units\"), activation='relu'))\n",
    "model.add(keras.layers.Dense(units=fcbest_hps.get(\"units1\"), activation='relu'))\n",
    "model.add(keras.layers.Dense(units=fcbest_hps.get(\"units2\"), activation='relu'))\n",
    "model.add(keras.layers.Dense(units=fcbest_hps.get(\"units3\"), activation='relu'))\n",
    "model.add(keras.layers.Dense(units=fcbest_hps.get(\"units4\"), activation='relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(rate=fcbest_hps.get(\"drop\")),)\n",
    "    \n",
    "model.add(keras.layers.Dense(7))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=fcbest_hps.get(\"learning_rate\")),\n",
    "                loss=keras.losses.CategoricalHinge(),\n",
    "                metrics=['accuracy'])\n",
    "spectralcross = np.concatenate((zcross,spectral),axis=1)\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(spectralcross, Y, stratify=Y, test_size=0.2)\n",
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=60, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45231b66",
   "metadata": {},
   "source": [
    "## ZCross+Spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "id": "01414db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 2ms/step - loss: 1.1666 - accuracy: 0.2587\n",
      "[test loss, test accuracy]: [1.1665539741516113, 0.25868263840675354]\n"
     ]
    }
   ],
   "source": [
    "eval_result = model.evaluate(testX, testY)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa015f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c01ad0c5",
   "metadata": {},
   "source": [
    "### Chroma alone performs the best on a fully connect neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c118a0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
